\documentclass[a4paper,onecolumn,oneside,12pt,wide,floatssmall]{mwrep}
\usepackage{algorithm}
\usepackage{algpseudocode}
\usepackage{amsmath}
\usepackage{amsfonts}
\usepackage{amssymb}
\usepackage{amsthm}
\usepackage{float}
\usepackage{geometry}
\usepackage{listings}
\usepackage[pdftex]{color,graphicx}
\usepackage{polski}
\usepackage[pdftex, bookmarks=false]{hyperref}
\usepackage[table]{xcolor}
\usepackage{tikz}
\usepackage[T1]{fontenc}
\usepackage[utf8x]{inputenc}
\usepackage[sort, compress]{cite}
\makeatletter\renewcommand{\ALG@name}{Algorytm}

% interlinia 1,5
\linespread{1.3}

\newcommand{\norm}[1]{\lVert#1\rVert}
\def\url#1{{ \tt #1}}
\def\C{{\rm C}\,}
\def\E{{\rm E}\,}
\def\sgn{{\rm sgn}\,}

% marginesy
\textwidth\paperwidth
\advance\textwidth -55mm
\oddsidemargin-0.9in
\advance\oddsidemargin 33mm
\evensidemargin-0.9in
\advance\evensidemargin 33mm
\topmargin -1in
\advance\topmargin 25mm
\setlength\textheight{46\baselineskip}
\addtolength\textheight{\topskip}
\marginparwidth15mm

\clubpenalty=10000 % to kara za sierotki
\widowpenalty=10000 % nie pozostawia wdów
\brokenpenalty=10000 % nie dzieli wyrazów pomiędzy stronami
\sloppy

\tolerance4500
\pretolerance250
\hfuzz=1.5pt
\hbadness1450

% ŻYWA PAGINA
\renewcommand{\chaptermark}[1]{\markboth{\scshape\small\bfseries \
#1}{\small\bfseries \ #1}}
\renewcommand{\sectionmark}[1]{\markboth{\scshape\small\bfseries\thesection.\
#1}{\small\bfseries\thesection.\ #1}}
\newcommand{\headrulewidth}{0.5pt}
\newcommand{\footrulewidth}{0.pt}
\pagestyle{uheadings}

\theoremstyle{definition}
\newtheorem{defn}{Definicja}[section]
\newtheorem{conj}{Teza}[section]
\newtheorem{conjmain}{Teza}
\newtheorem{exmp}{Przykład}[section]

\theoremstyle{plain}% default
\newtheorem{thm}{Twierdzenie}[section]
\newtheorem{lem}[thm]{Lemat}
\newtheorem{prop}[thm]{Hipoteza}
\newtheorem*{cor}{Wniosek}

\theoremstyle{remark}
\newtheorem*{rem}{Uwaga}
\newtheorem*{note}{Uwaga}
\newtheorem{case}{Przypadek}

\definecolor{ListingBackground}{rgb}{0.95,0.95,0.95}

\begin{document}

\renewcommand*\lstlistingname{Wydruk}
\renewcommand*\lstlistlistingname{Spis wydruków}

\pagenumbering{roman}
\renewcommand{\baselinestretch}{1.0}
\raggedbottom
\input {title}

\tableofcontents

\newpage
\pagenumbering{arabic}
\setcounter{page}{1}

\chapter{Wstęp}

Ewolucja różnicowa (ang. differential evolution, DE) jest algorytmem przeznaczonym 
do optymalizacji globalnej 
w przestrzeniach ciągłych \cite{storn}. Optymalizacja globalna jest zadaniem znalezienia ekstremum 
globalnego (minimum bądź maksimum) pewnej funkcji, zwanej funkcją celu, bądź funkcją oceny. 
Funkcja ta jest ,,czarną skrzynką'', tzn. jej dokładna postać analityczna w ogólnym przypadku 
jest nieznana. W związku z tym analityczne wyznaczenie ekstremów często jest niemożliwe. W przypadku 
skomplikowanych, wielowymiarowych funkcji znalezienie dokładnego optimum może być bardzo czasochłonne, 
a w praktyce wręcz niewykonalne. Wówczas celem jest znalezienie możliwie najlepszego rozwiązania
suboptymalnego. Algorytm optymalizujący wybiera pewne rozwiązanie, następnie poddaje to rozwiązanie 
ocenie przez funkcję oceniająca, w wyniku czego otrzymuje liczbową wartość (jakość, dobroć) 
podanego rozwiązania. 

Przykładowo, ,,czarną skrzynką'' może być program (symulator), który po podaniu 
parametrów na wejście przeprowadza jeden eksperyment i zwraca np. liczbę watów wygenerowanych
przez panele słoneczne. Parametrami wejściowymi mogą być kąty nachylenia i prędkości obrotu paneli. 
Panele te umieszczone są na statku kosmicznym, który krąży po orbicie ziemskiej.
Przez to niektóre panele rzucają cień na inne, jeśli są źle ustawione. Panele również nie mogą być 
zbyt długo wystawione na działanie promeini słonecznych, ponieważ może dojść do uszkodzenia. 
To wprowadza kłopotliwe ograniczenia w przestrzeni przeszukiwań.
Symulator przeprowadza kosztowne obliczenia, żeby zwrócić całkowitą liczbę watów wygerenowanych
przez panele w ciągu pełnego obiegu wokół Ziemi. 



Algorytm ewolucji różnicowej może mieć wiele postaci (wariantów), w zależności od używanych
operatorów genetycznych -- mutacji oraz krzyżowania. 
W tej pracy skoncentrowano się na odpowiednim doborze operatora mutacji, 
tak aby jakość zwracanych wyników była jak największa.

\section{Cel pracy}

Podstawowym celem pracy była poprawa efektywności algorytmu ewolucji różnicowej.
Wiązało się to ze szczegółową analizą i przetestowaniem istniejących wariantów algorytmu,
a następnie zapronowaniem lepszej metody. Nowy wariant, nazwany DE/mid, został porównany 
do dwóch znanych odmian -- DE/rand oraz DE/best. 
Wyniki eksperymentów wskazały na znaczną przewagę DE/mid na 6 z 7 wielowymiarowych funkcji testowych.

\section{Układ pracy}

Rozdział pierwszy stanowi wprowadzenie do tematu pracy, definiując jej cel oraz podstawowe pojęcia.
W rozdziale drugim wyjaśniono podstawy działania algorytmów ewolucyjnych. 
Następnie przedstawiono dwa znane warianty ewolucji różnicowej oraz 
zaprezentowano nowy wariant ewolucji różnicowej nazwanej DE/mid. W części tej wyprowadzono również
wzory na istotny parametr ewolucji różnicowej -- współczynnik skalujący. 
W~rodziale czwartym opisano metodykę testowania, jakich parametrów oraz jakich funkcji testowych 
używano.
Wyjaśniono również w jaki sposób testy zostały 
zaimplementowane oraz jak została sprawdzona ich poprawność. Na zakończenie rodziału opisano
metodę porównywania wyników eksperymentów -- zwrócono uwagę na testy istotności 
statystycznej, dystrybuanty empiryczne oraz liczbę osobników poza obszarem przeszukiwań.
W rozdziale piątym zaprezentowano w sposób zbiorczy wyniki wszystkich eksperymentów,
przedstawiono również wnioski z nich płynące. 
Pracę zamyka podsumowanie oraz wskazanie możliwych kierunków dalszego rozwoju.

\section{Słownik pojęć i oznaczeń}

\begin{itemize}
 \item[] Osobnik, punkt, wektor, rozwiązanie -- podstawowy element przestrzeni przeszukiwań, 
przetwarzany przez algorytm ewolucji różnicowej.
 \item[] Populacja -- zbiór osobników, przetwarzany w pojedynczej iteracji algorytmu.
 \item[] DE (ang. Differential Evolution) -- ewolucja różnicowa.
 \item[] DE/rand -- klasyczny wariant DE przesuwający losowy punkt w mutacji.
 \item[] DE/best -- znany wariant DE przesuwający najlepszy punkt w mutacji.
 \item[] DE/mid -- zaproponowany wariant DE przesuwający punkt środkowy w mutacji.
 \item[] BBOB (ang. Black-Box Optimization Benchmarking) -- nazwa warsztatów poświęconych testowaniu algorytmów optymalizacji. 
Warsztaty te odbywały się w latach 2009, 2010, 2012 oraz 2013. Zmieniały się algorytmy startujące w konkursie, nie zmieniał
się sposób ich testowania (opisany w rozdziale \ref{sec:zestaw}).
 \item[] $k$ -- liczba wektorów różnicowych używanych w mutacji.
 \item[] $\mu$ -- liczba osobników w populacji.
 \item[] $D$ -- liczba cech osobnika, również wymiar optymalizowanej funkcji.
 \item[] $P^t$ -- populacja o numerze $t$.
 \item[] $P_i^t$ -- osobnik o indeksie $i$ w populacji $P^t$.
 \item[] $O^t$ -- populacja potomków u numerze $t$.
 \item[] $O_i^t$ -- osobnik o indeksie i w populacji potomków $O^t$.
 \item[] $u_i$ -- mutant osobnika o indeksie $i$ w populacji.
 \item[] $o_i$ -- potomek osobnika o indeksie $i$ w populacji.
 \item[] $F$ (ang. Factor) -- współczynnik skalujący, parametr ewolucji różnicowej sterujący zasięgiem mutacji.
 \item[] $CR$ (ang. Crossover Rate) -- prawdopodobieństwo krzyżowania, parametr algorytmów ewolucyjnych sterujący częstością krzyżowań.
 \item[] $f$ -- funkcja celu, funkcja przystosowania, optymalizowana funkcja, funkcja testowa.
 \item[] $f_\text{opt}$ -- optymalna wartość $f$, tzn. minimum podczas minimalizacji lub maksimum podczas maksymalizacji.
 \item[] $\C[u]$ -- macierz kowariancji rozkładu zmiennej losowej $u$.
\end{itemize}

\chapter{Ewolucja różnicowa}

\section{Algorytm ewolucyjny}

W algorytmie ewolucyjnym punkty przestrzeni przeszukiwań
reprezentowane są jako tablice liczb rzeczywistych \cite{jarabas}. 
Zasadę działania algorytmu ewolucyjnego przedstawiano na listingu \ref{algorithm:ea}.

\begin{algorithm}[H]
\caption{Algorytm ewolucyjny}
\label{algorithm:ea}
\begin{algorithmic}[1]
\State Inicjalizacja parametrów $CR, \mu$
\State Inicjalizacja populacji początkowej $P^1 \gets \{P^1_1, P^1_2, \ldots, P^1_{\mu}\}$
\State $t \gets 1$
\While{kryterium stopu nie zostało spełnione}
  \ForAll{$i \in \{ 1, 2, \ldots, \mu \}$}
    \If{$\mathcal{U}(0, 1) < CR$}
      \State $O^t_i \gets$ mutacja(krzy{\.z}owanie(selekcja($P(t)$), selekcja($P(t)$)))
    \Else
      \State $O^t_i \gets$ mutacja(selekcja($P(t)$))
    \EndIf
  \EndFor
  \State $P^{t+1} \gets$ sukcesja$(P^t, O^t)$
  \State $t \gets t+1$
\EndWhile
\end{algorithmic}
\end{algorithm}

Algorytm ewolucyjny w podstawowej postaci ma dwa parametry wejściowe.
Są nimi: $CR$~--~prawdopodobieństwo krzyżowania oraz $\mu$ -- liczba osobników w populacji.
Po nadaniu wartości początkowych parametrom, następuja inicjalizacja populacji początkowej. 
Jeśli zupełnie nie wiadomo, gdzie może znajdować się poszukiwane optimum, 
populację początkową generuje się w sposób losowy. 

Po inicjalizacji populacji początkowej następuje główna pętla programu. 
Wykonywana jest ona dopóty, dopóki nie zajdzie tzw. kryterium stopu. 
Kryterium stopu może być np. maksymalna liczba iteracji lub wywołań funkcji oceny.
Algorytm można zatrzymać również wtedy, gdy znalezione rozwiązanie jest zadowalające. 
Innym kryterium stopu jest przerwanie algorytmu, gdy wszystkie 
punkty w populacji zdegenerują się do jednego. Wówczas
każda kolejna populacja będzie bardzo podobna do poprzedniej. 
W związku z tym nie ma sensu dalsze iterowanie, 
można wówczas zakończyć algorytm i np. uruchomić go jeszcze raz z innego miejsca
albo z innymi paramerami.

W głównej pętli programu przetwarzana jest bieżaca populacja. 
Wykonywana jest kolejna pętla, tym razem iterująca kolejno po wszystkich osobnikach 
(rozwiązaniach) z bieżącej populacji. W niej losowana jest liczba od 0 do 1 i jeśli jest ona mniejsza
od ustalonego $CR$ -- dokonywana jest selekcja dwóch punktów, ich krzyżowanie, a następnie mutacja.
W przeciwnim przypadku krzyżowanie jest pomijane.
Po wygenerowaniu populacji potomków $O^t$ następuje sukcesja, czyli wybór punktów, które przechodzą
do następnej populacji $P^{t+1}$. Zwiększany jest licznik populacji (czasu) $t$,
sprawdzane jest kryterium stopu i jeśli nie jest spełnione --
wykonywana jest kolejna pętla.
 
Kolejnym problemem jest dobór parametrów dla algorytmu ewolucyjnego. 
Istnieją pewne porady, np. stosowana w tej pracy reguła $\mu = 10D$, 
jednak w ogólnym przypadku nie wiadomo jaki rozmiar populacji czy jakie prawdopodobieństwo
krzyżowania $CR$ sprawdzi się najlepiej. Z pomocą przychodzą
metody samoadaptacji, które same dostrajają parametry algorytmu w zależności od stanu w jakim się
znajdują, np. w~zależności od zawartości populacji \cite{brest}.
Niewątpliwą zaletą ewolucji różnicowej jest jej pewien rodzaj samoadaptacji, dzięki
czemu posiada stosunkowo małą liczbę parametrów do strojenia w porównaniu do innych algorytmów
optymalizacji.

\section{Metaheurystyki}

\section{Algorytm ewolucji różnicowej}

Ewolucja różnicowa jest algorytmem ewolucyjnym służącym do optymalizacji
w przestrzeniach ciągłych zaproponowanym w 1995 roku. \cite{storn} Sposób działania algorytmu
przedstawiono na listingu \ref{algorithm:de}.

Kolejność wykonywania operatorów genetycznych (mutacji oraz krzyżowania) w~ewolucji różnicowej
jest inna niż w algorytmie ewolucyjnym.
W algorytmie ewolucyjnym najpierw wykonywane jest krzyżowanie, potem mutacja.
W ewolucji różnicowej -- na odwrót.

\usetikzlibrary{decorations.pathreplacing,calc}
\newcommand{\tikzmark}[1]{\tikz[overlay,remember picture] \node (#1) {};}

\newcommand*{\AddNote}[4]{%
    \begin{tikzpicture}[overlay, remember picture]
        \draw [decoration={brace,amplitude=0.5em},decorate,thick]
            ($(#3)!(#1.north)!($(#3)-(0,1)$)$) --  
            ($(#3)!(#2.south)!($(#3)-(0,1)$)$)
                node [align=center, text width=2.5cm, pos=0.5, anchor=west] {#4};
    \end{tikzpicture}
}%

\begin{algorithm}[H]
\caption{Ewolucja różnicowa}
\label{algorithm:de}
\begin{algorithmic}[1]
\State Inicjalizacja parametrów $CR, F, \mu$
\State Inicjalizacja populacji początkowej $P^1 \gets \{P^1_1, P^1_2, \ldots, P^1_\mu\}$
\State $t \gets 1$
\While{kryterium stopu nie zostało spełnione}
  \ForAll{$i \in \{ 1, 2, \ldots, \mu \}$}
    \State $v_i \gets$ selekcja$(P^t_i)$
    \State $u_i \gets$ mutacja$(v_i, P^t_i, F)$ 
    \State $o_i \gets$ krzy{\.z}owanie$(u_i, P_i^t, CR)$    
    \If{$f(o_i) \le f(P_i^t)$}  \tikzmark{top} \tikzmark{right} 
      \State $P_i^{t+1} \gets o_i$     
    \Else 
      \State $P_i^{t+1} \gets P_i^{t}$
    \EndIf \tikzmark{bottom}
  \EndFor
\EndWhile \\
\Return arg min$_i f(P^t_i)$ \AddNote{top}{bottom}{right}{~~sukcesja}
\end{algorithmic}

\end{algorithm}

Kolejną różnicą jest konkretyzacja operatora sukcesji. Po wygenerowaniu potomka
$o_i$ jest on od razu porównywany ze swoim rodzicem. Jeśli okaże się lepszy -- zastępuje go
w~kolejnej populacji. Powoduje to silny nacisk selektywny -- dalej przechodzą jedynie dzieci
lepiej przystosowane od swoich rodziców.

\subsection{Selekcja}

Operator selekcji na wejściu otrzymuje bieżącą populację i zwraca jedno rozwiązanie.
W tej pracy zwrócono szczególną uwagę na właściwy dobór tego operatora.
Skrót DE/rand oznacza ewolucję różnicową z losowym typem selekcji, tzn. wybierany jest jeden
losowy punkt z całej populacji zgodnie z rozkładem jednostajnym.
Inne warianty ewolucji różnicowej -- DE/best oraz zaproponowany w tej pracy DE/mid, 
różnią się właśnie operatorem selekcji. Inne możliwe typy selekcji, 
np. rand-to-best, current-to-best oraz current-to-rand,
opisano w \cite{zaharie}.

\subsection{Mutacja}

Operator mutacji na wejściu otrzymuje jedno rozwiązanie i zwraca również jedno.
Jako trzeci człon skróconej nazwy algorytmy
podawana jest liczba wektorów różnicowych używanych w mutacji -- $k$.
Np. DE/rand/1 oznacza losową selekcję z jednym wektorem różnicowym w mutacji.
Ten podstawowy wariant algorytmu nazywany jest również ,,klasyczną'' ewolucją różnicową. 

\subsection{Krzyżowanie}

Operator krzyżowania na wejściu otrzymuje dwa rozwiązania rodzicielskie, a zwraca jedno potomne. 
W ewolucji różnicowej jednym z rodziców jest mutant. Istnieje wiele rodzajów krzyżowania, 
przykładowe to:
\begin{itemize} 
 \item[$\bullet$]  Krzyżowanie wymieniające. 
Część elementów w tablicy rozwiązania jest brana od jednego z rodziców, 
pozostała część jest brana od drugiego. Ten sposób krzyżowania (w wariancie bin)
został wykorzystany w tej pracy.
 \item[$\bullet$] Krzyżowanie uśredniające. 
Potomkiem jest rozwiązanie znajdujące się dokładnie w~połowie odległości pomiędzy rodzicami.
\end{itemize} 

\section{Techniki uwzględniania ograniczeń}

\chapter{Algorytm DE/mid -- nowy wariant ewolucji różnicowej}

\section{Analiza porównawcza macierzy kowariancji różnych wariantów ewolucji różnicowej}

W poniższych podrozdziałach przedstawiono operatory mutacji dla DE/rand/k, DE/best/k, DE/mid/k 
oraz wyprowadzono wzory na współczynniki skalujące $F$. We wzorach, dla zwiększenia
czytelności, pomijano indeks górny $i$ dla populacji, np. stosowano zapis $P_{i_1}$ 
zamiast $P_{i_1}^i$.

\subsection{DE/rand -- klasyczny wariant ewolucji różnicowej}
\label{chap:de_rand} 

W DE/rand/1, mutant $i$-tego osobnika w populacji $P$~o~$\mu$~osobnikach powstaje w następujący sposób 
\cite{decomposition}:
\begin{equation} \label{eq:derand1}
u_i = P_{i_1} + F(P_{i_2} - P_{i_3})
\end{equation}

$i_1, i_2, i_3$ to indeksy wylosowane zgodnie z rozkładem jednostajnym ze zbioru \\ 
$\{1, 2, \dots, \mu\}$. Zatem $P_{i_1}, P_{i_2}, P_{i_3}$ to rozwiązania wylosowane zgodnie z 
rozkładem jednostajnym z populacji $P$.
$F\in\mathbb{R_+}$ to współczynnik skalujący zasięg mutacji. 

DE/rand/1 można uogólnić na DE/rand/k, w którym dodawanych jest
$k \in \mathbb{N}$ wektorów różnicowych:
\begin{equation} \label{eq:derand}
u_i' = P_{i_1} + F_k\sum\limits_{j=1}^k (P_{i_{2j}} - P_{i_{2j+1}})
\end{equation}

Podobnie jak poprzednio, $i_1, i_2, \dots i_{2k+1}$ to indeksy wylosowane zgodnie z rozkładem jednostajnym ze zbioru 
$\{1, 2, \dots, \mu\}$. Zatem $P_{i_1}, P_{i_2}, \dots, P_{2k+1}$ to rozwiązania wylosowane zgodnie z rozkładem 
jednostajnym z populacji $P$. $F_k\in\mathbb{R_+}$ to współczynnik skalujący dla DE/rand/k. 

Żeby zasięg mutacji nie zmieniał się wraz ze wzrostem $k$, 
macierz kowariancji mutanta $u_i'$ musi być taka sama jak macierz kowariancji mutanta $u_i$.
Można to osiągnąć tak dobierając $F_k$, aby było spełnione równanie:
\begin{equation} \label{eq:kowariancje}
\C[u_i] = \C[u_i']
\end{equation}

$\C[u_i]$ jest macierzą kowariancji rozkładu zmiennej losowej $u_i$.
Rozwijając lewą stronę równania \eqref{eq:kowariancje} i korzystając z tego, że osobniki są niezależne od siebie:
\begin{align*}
\C[u_i] \overset{(\ref{eq:derand1})}{=} \C[P_{i_1} + F(P_{i_2} - P_{i_3})] = \C[P_{i_1}] + F^2(\C[P_{i_2}] + \C[P_{i_3}])
\end{align*}

Każdy osobnik ma taki sam rozkład prawdopodobieństwa. 
Macierz kowariancji punktów losowanych z $P$ ma teorytyczną wartość $\C[P]$,
równej empirycznej macierzy kowariancji populacji $P$.
Zatem dla DE/rand/1 macierz kowariancji mutanta wynosi:
\begin{equation} \label{eq:macierz_kow_mutanta}
\C[u_i] = \C[P] + F^2(\C[P] + \C[P]) = \C[P](2F^2 + 1)
\end{equation}

Rozwijając prawą stronę równania \eqref{eq:kowariancje}, w przypadku DE/rand/k macierz kowariancji mutanta wynosi:
\begin{multline} \label{eq:macierz_kow_mutanta_prim}
\C[u_i'] \overset{(\ref{eq:derand})}{=} \C[P_{i_1} + F_k\sum\limits_{j=1}^k (P_{i_{2j}} - P_{i_{2j+1}})] 
= \C[P_{i_1}] + F_k^2\C[\sum\limits_{j=1}^k (P_{i_{2j}} - P_{i_{2j+1}})] \\
= \C[P_{i_1}] + F_k^2\C[\sum\limits_{j=2}^{2k+1} P_{i_{j}}] = \C[P](2kF_k^2 + 1)
\end{multline}

Przyrównując \eqref{eq:macierz_kow_mutanta} oraz \eqref{eq:macierz_kow_mutanta_prim}:
\begin{align*}
\C[P](2F^2 + 1) = \C[P](2kF_k^2 + 1)
\end{align*}

Przy założeniu, że $\C[P] \neq \textbf{0}$:
\begin{align*}
F^2 = kF_k^2
\end{align*}

Obie strony są nieujemne, więc ostatecznie:
\begin{align*}
F_k = \frac{F}{\sqrt{k}}
\end{align*}

Zatem, dla zachowania zasięgu mutacji, współczynik skalowania dla DE/rand/k powinien być  
$\sqrt{k}$ razy mniejszy od współczynnika skalowania dla DE/rand/1. Po wstawieniu powyższego wyniku
do \eqref{eq:derand}, wzór na mutanta dla DE/rand/k można zapisać nastepująco:

$$ u_i' = P_{i_1} + \frac{F}{\sqrt{k}}\sum\limits_{j=1}^k (P_{i_{2j}} - P_{i_{2j+1}}) $$

\subsection{DE/rand/$\infty$}
\label{sub:de_rand_inf}

Zgodnie z centralnym twierdzeniem granicznym, część $\frac{1}{{\sqrt{k}}}\sum\limits_{j=1}^k (P_{i_{2j}} - P_{i_{2j+1}})$ 
zbiega według rozkładu do $\mathcal{N}(0, \C[P])$ gdy $k \to \infty$. 
Dzięki temu, równanie mutanta DE/rand/$\infty$ można zapisać jako:
\begin{align*}
u_i^\infty = P_{i_1} + F_\infty \cdot v_\infty
\end{align*}

gdzie $v_\infty \sim \mathcal{N}(0, \C[P])$. Współczynnik $F_\infty$ zapewniający
równość macierzy kowariancji dla wariatnów DE/rand/1 oraz DE/rand/$\infty$
można wyznaczyć następująco:
\begin{align*}
\C[u_i] = \C[P_{i_1} + F_\infty \cdot v_\infty] \overset{(\ref{eq:macierz_kow_mutanta})}{=} \C[P](2F^2 + 1) \\
\C[P] + \C[F_\infty \cdot v_\infty] = \C[P](2F^2 + 1) \\
\C[F_\infty \cdot v_\infty] = 2F^2\C[P] \\
F_\infty^2 \C[P] = 2F^2\C[P] \\
F_\infty^2 = 2F^2 \\
F_\infty = \sqrt{2}F
\end{align*}

\subsection{DE/best}

DE/best jest podobny do DE/rand, opisanego w poprzednich podrozdziałach.
Jedyną różnicą jest wybieranie najlepszego punktu w selekcji zamiast losowego. Obliczane są wartości
funkcji oceny dla wszystkich osobników w populacji i punkt o~najmniejszej wartości (przy minimalizacji)
jest traktowany jako najlepszy.

\section{Analiza rozkładu mutantów różnych wariantów ewolucji różnicowej}

\subsection{Wpływ symetrii rozkładu mutantów}

Wektor różnicowy dla każdego wariantu algorytmu ma postać
$ F_k\sum\limits_{j=1}^k (P_{i_{2j}} - P_{i_{2j+1}}) $. Zmienna losowa o tej postaci
ma symetryczny rozkład prawdopodobieństwa, dążacy do rozkładu Gaussa, gdy $k$ dąży do 
nieskończoności. W przypadku DE/mid (rysunek \ref{fig:sym_mid}) 
oraz DE/best (rysunek \ref{fig:sym_best}),
przesuwany jest jeden ustalony punkt (środkowy albo najlepszy), 
a więc rozkład mutanta dalej jest symetryczny.
Natomiast w przypadku DE/rand (rysunek \ref{fig:sym_rand}) 
przesuwany jest punkt losowy, co zaburza symetrię rozkładu mutanta.
Wykresy przedstawiają histogram częstości występowania
mutantów (zaznaczonych okręgami) w zależności od ich jedynej cechy $x$.
Populację początkową przedstawiono na rysunku \ref{fig:sym_none}.
Jest nią 6 punktów o wartościach $x$ równych kolejno $-0.5, -0.4, 0, 0.25, 0.3, 3$.
Punkty te dobrano specjalnie w taki sposób, aby można było zaobserwować asymetrię
rozkładu DE/rand -- pięć pierwszych punktów znajduje się w grupie po lewej stronie, 
po prawej natomiast znajduje się tylko jeden punkt.
Współczynnik skalujący $F$ wynosił 1, dla uproszczenia.
Dla DE/best przyjęto, że najlepszym punktem był punkt o $x = 0$.

\begin{figure}[H]
\centering
\includegraphics[width=.65\textwidth]{img/none}
\caption{6 punktów populacji początkowej} 
\label{fig:sym_none}
\end{figure}

\begin{figure}[H]
\centering
\includegraphics[width=.65\textwidth]{img/rand}
\caption{Histogram mutantów dla DE/rand/1} 
\label{fig:sym_rand}
\end{figure}

\begin{figure}[H]
\centering
\includegraphics[width=.65\textwidth]{img/mid}
\caption{Histogram mutantów dla DE/mid/1} 
\label{fig:sym_mid}
\end{figure}

\begin{figure}[H]
\centering
\includegraphics[width=.65\textwidth]{img/best}
\caption{Histogram mutantów dla DE/best/1} 
\label{fig:sym_best}
\end{figure}

W ostatecznej klasyfikacji DE/mid (symetryczy) zajął pierwsze miejsce, 
DE/rand (niesymetryczny) -- drugie, natomiast DE/best (symetryczny) -- trzecie.
Dlatego na podstawie przeprowadzonych eksperymentów nie można określić czy symetria rozkładu 
mutantów miała istotny wpływ na jakość otrzymywanych wyników. 

\subsection{Wpływ liczby par wektów różnicowych $k$}

Algorytmy dla $k = 6$ zachowywały się identycznie jak dla $k = \infty$. Jest to wynik zgodny
z rozważaniami teoretycznymi przedstawionymi w \cite{decomposition}. 
W przypadku $k = 6$ w mutacji sumujemy aż 12 wektorów różnicowych,
czyli 12 niezależnych zmiennych. Na mocy centralnego twierdzenia granicznego,
im większe $k$, tym rozkład sumy coraz bardziej przypomina wielowymiarowy rozkład normalny,
a zatem przypadek $k=\infty$. Zostało to dokładnie opisane w rozdziale \ref{sub:de_rand_inf}.
Wydajne losowanie zmiennych o rozkładzie wielowymiarowym normalnym, o zadanej
macierzy kowariancji, było zadaniem trudnym do implementacji w Javie.
Dla każdego populacji, najpierw trzeba wyznaczyć jej macierz kowariancji, a następnie macierz
tą trzeba rozłożyć. Macierz kowariancji często nie była dodatnie określona, 
dlatego dekompozycja Choleskiego nie mogła zostać użyta.
Z tego powodu wykorzystano rozkład macierzy na wektory i wartości własne -- metodę ogólniejszą, jednak
bardziej kosztowną obliczeniowo. Operator mutacji w DE/mid/$\infty$ wykonuje się o wiele
wolniej od operatora mutacji w DE/mid/6, a~ponieważ mutacja jest powtarzana miliony
razy, ogólna wydajność DE/mid/$\infty$ prezentuje się zdecydowanie najgorzej na tle pozostałych
algorytmów. Z powodu braku różnic w jakości otrzymywanych rozwiązań,
 algorytmy z $k = 6$ są lepszym wyborem z praktycznego punktu widzenia.

Natomiast ogólnie najlepszą wartością dla $k$ było 1. Im wyższe $k$, tym algorytm spisywał się gorzej.
Relacja pomiędzy DE/mid, DE/rand oraz DE/best, dla większości funkcji pozostawała taka sama, 
niezależnie od $k$, tzn. DE/mid był zazwyczaj najlepszy, DE/rand drugi, zaś DE/best najgorszy
dla takiego samego $k$. Rozkład mutantów dla $k = 1$ nie przypomina rozkładu normalnego, 
natomiast dla $k = 6$ już upodabnia się do niego, zaobserwować to można na rysunkach \ref{fig:10k}. 
Przedstawiają one 10 tysięcy mutantów wygenerowanych z tej samej, dwuwymiarowej populacji początkowej.
Po lewej stronie użyto mutacji DE/rand/1, po prawej DE/rand/6. Wspołczynnik skalujący $F$ dla 
DE/rand/1 wynosił 1, dla DE/rand/6 (zgodnie z tabelą \ref{table:wspolczynniki}) $\frac{1}{\sqrt{6}}$.
Populacja początkowa przedstawiona na rysunku \ref{fig:10k_start} zawierała 20 punktów, 
przy czym 18 z nich leżało niemal w jednej linii na osi $y = 0$. 
Jedynie 2 punkty wprowadzały większą różnorodność 
w wymiarze $y$: jeden o $y = 2.5$ oraz jeden o~$y = -2.5$. Liczba mutantów,
dla których $|y| > 5$, znacznie się różniła. Dla DE/rand/1 było to 75 mutantów,
natomiast dla DE/rand/6 -- 19. Obliczenia
powtórzono i uśredniono wielokrotnie, aby wyelimonować czynnik losowy. Mutanty, które sięgają
na tyle daleko, że umożliwiają wyjście z lokalnego optimum, są bardzo cenne. 
Ciągłe próbkowanie okolicznych, już zbadanych obszarów, na ogół nie przynosi korzyści. 

\begin{figure}[H]
\centering
\includegraphics[width=.5\textwidth]{img/pop} 
\caption{20 punktów populacji początkowej} 
\label{fig:10k_start}
\end{figure}

\begin{figure}[H]
\centering
\mbox{
\includegraphics[width=.5\textwidth]{img/pop_10000_r1}
\includegraphics[width=.5\textwidth]{img/pop_10000_r6}  
}
\caption{Po lewej 10 tysięcy mutantów dla DE/rand/1. Po prawej 10 tysięcy mutantów dla DE/rand/6.} 
\label{fig:10k}
\end{figure}

Ponadto, podczas mutacji DE/rand/1 zazwyczaj powstawał jeden mutant o $y > 7.5$. 
Żeby tak się zdarzyło, najpierw wylosowany musi zostać osobnik $P_{i_1}$ o~$y=2.5$.
Prawdopodobieństwo takiego zdarzenia wynosi $\frac{1}{20}$.
Następnie musi zostać wylosowany wektor różnicowy o~długości 5. Tylko jedna para punktów 
dawała taki wektor --
pierwszy punkt o $y = 2.5$, drugi punkt o $y = -2.5$. 
Prawdopodobieństwo wylosowania takiej pary wynosiło $\frac{1}{400}$. 
Zatem łączne prawdopodobieństwo powstania mutanta o~
$y~=~7.5$ dla DE/rand/1 wynosiło $\frac{1}{8000}$. 

Dla DE/rand/6 prawdopodobieństwo powstania mutanta o $y > 7.5$ jest znacznie mniejsze.
W celu oszacowania, przyjęto dla uproszczenia, że rozkład prawdopodobieństwa mutantów dla
DE/rand/6 jest wystarczająco zbliżony do DE/rand/$\infty$.
Zgodnie z opisem w rozdziale \ref{sub:de_rand_inf}, na początku wylosowany musi zostać osobnik 
$P_{i_1}$ o $y = 2.5$. Następnie musi zostać wygenerowana zmienna losowa 
$v_\infty$ o rozkładzie normalnym, której wartość w wymiarze $y$
przekroczy $5\sqrt{6} \approx 12.25$. Macierz kowariancji populacji początkowej  
$\C[P]$ wynosiła $\left[ \begin{array}{ c c }
  35.1798210 & -0.2902915 \\
  -0.2902915 &  0.6719438
\end{array} \right]$.
Wektor średnich~$m=\left[ \begin{array}{ c c } 10.463749675 & 0.004673205 \end{array} \right]$.
Prawdopodobieństwo wylosowania zmiennej o~$y > 5\sqrt{6}$ wynosi 
$1 - F(\infty, 5\sqrt{6}) \approx 0$.\footnote{Wartość ta jest tak mała,
że w reprezentacji liczb podwójnej precyzji użytych przy wywołaniu funkcji \texttt{pmvnorm} języka R
została uznana za 0.} $F$ to
dystrybuanta wielowymiarowego rozkładu $\mathcal{N}(m, \C[P])$.
Całkowite prawdopodobieństwo wylosowanie mutanta o $y > 7.5$ dla DE/rand/6 jest zatem 
znacznie mniejsze w porównaniu do
$\frac{1}{8000}$ dla DE/rand/1.
Gdyby pierwszym wylosowanym punktem $P_{i_1}$ nie był ten o $y = 2.5$, tylko np. któryś 
z punktów leżących blisko osi $y = 0$, wówczas mutacja DE/rand/6 musiałaby wygenerować zmienną
o wartości $y$ większej niż $7.5\sqrt{6} \approx 18.37$. 
Prawdobieństwo takiego zdarzenia również jest bliskie zeru.

\section{Algorytm DE/mid -- nowy wariant ewolucji różnicowej}
\label{chap:de_mid}

\subsection{Schemat algorytmu}

W DE/mid/k mutant powstaje w następujący sposób:

\begin{equation} \label{eq:demid}
u_i'' = m + F_m\sum\limits_{j=1}^k (P_{i_{2j}} - P_{i_{2j+1}})
\end{equation}

Jedyną różnicą w porównaniu do DE/rand/k opisanego w rozdziale \ref{chap:de_rand} jest $m$, czyli punkt środkowy populacji:
\begin{equation} \label{eq:midpoint}
m = \frac{1}{\mu}\sum\limits_{j=1}^n P_j
\end{equation}

$F_m\in\mathbb{R_+}$ jest współczynnikiem skalującym dla DE/mid/k, analogicznym do $F$ 
w przypadku DE/rand/1. 

\subsection{Rozład generowanych mutantów}

Żeby macierz kowariancji populacji w DE/mid była taka sama jak w~DE/rand/1, 
macierz kowariancji mutanta $u_i''$ musi być taka sama jak macierz kowariancji mutanta $u_i$.
Można to osiągnąć tak dobierając $F_m$, żeby było spełnione równanie:
\begin{equation} \label{eq:rownanie}
\C[u_i] = \C[u_i'']
\end{equation}

Rozwijając prawą stronę równania (\ref{eq:rownanie}):
\begin{align*}
\C[u_i''] \overset{\eqref{eq:demid}}{=} \C[m + F_m\sum\limits_{j=1}^k (P_{i_{2j}} - P_{i_{2j+1}})] \\
\overset{(\ref{eq:midpoint})}{=} \C[\frac{1}{\mu}\sum\limits_{j=1}^\mu P_j] + F_m^2\C[\sum\limits_{j=1}^k (P_{i_{2j}} - P_{i_{2j+1}})] 
= \frac{1}{\mu^2}\mu\C[P] + F_m^2\C[\sum\limits_{j=2}^{2k+1} P] = \C[P](2kF_m^2 + \frac{1}{\mu})
\end{align*}

Podstawiając do (\ref{eq:rownanie}):
\begin{align*}
\C[P](2F^2 + 1) = \C[P](2kF_m^2 + \frac{1}{\mu})
\end{align*}

Przy założeniu, że $\C[P] \neq \textbf{0}$:
\begin{align*}
2F^2 + 1 = 2kF_m^2 + \frac{1}{\mu} \\
F_m^2 = \frac{2F^2 + 1 - \frac{1}{\mu}}{2k}
\end{align*}

Obie strony są nieujemne, więc:
\begin{align} \label{eq:a}
F_m\ = \sqrt{\frac{2F^2 + 1 - \frac{1}{\mu}}{2k}}
\end{align}

Przyjmując $F=0.9$ z (\ref{eq:a}) bezpośrednio wynika, że $F_m \approx 1.14$ dla $k=1$ i $\mu\to\infty$.
W~DE/mid w selekcji wybierany jest punkt środkowy $m$ zamiast losowo wybranego osobnika $P_{i_1}$.
Punkt środkowy jest mniej zmienny niż losowy, 
tzn. norma macierzy kowariancji punktu środkowego jest mniejsza niż norma macierzy kowariancji 
losowego osobnika. Przy liczbie osobników dążacej do nieskończoności, macierz kowariancji
punktu środkowego dąży do macierzy wypełnionej samymi zerami, natomiast macierz kowariancji
losowego punktu dąży do macierzy kowariancji populacji. Formalnie: 
$\lim_{n\to\infty} \C[m] = \textbf{0}$, natomiast $\lim_{n\to\infty} \C[P_{i_1}] = \C[P]$.
Dlatego DE/mid/k zawsze potrzebuje większego współczynnika skalującego niż DE/rand/k.

Przechodząc z $k$ do nieskończoności, równanie mutanta DE/mid/$\infty$ można zapisać
w następujący sposób:
\begin{align*}
u_i'^\infty = m + F_{\infty_m} \cdot v_\infty
\end{align*}

Gdzie współczynnik skalujący $F_{\infty_m}$ jest równy $\sqrt{2F^2 + 1 - \frac{1}{\mu}}$, ponieważ:
\begin{align*}
\C[u_i'] = \C[m + F_{\infty_m} \cdot v_\infty] \overset{(\ref{eq:macierz_kow_mutanta})}{=} \C[P](2F^2 + 1) \\
\C[m] + C[F_{\infty_m} \cdot v_\infty] = \C[P](2F^2 + 1) \\
\frac{C[P]}{\mu} + F_{\infty_m}^2 C[P] = \C[P](2F^2 + 1) \\
F_{\infty_m} = \sqrt{2F^2 + 1 - \frac{1}{\mu}}
\end{align*}

\section{Współczynniki skalujące}

Tabela \ref{table:wspolczynniki} podsumowuje wyprowadzone współczynniki skalujące dla wszystkich
wariantów algorytmów analizowanych w tej pracy. Używając tych współczynników macierz kowariancji
mutantów dla każdego z algorytmów będzie równa macierzy kowariancji mutantów dla DE/rand/1.

Współczynniki skalujące dla DE/best są identyczne jak dla DE/mid, ponieważ w obu tych wariantach,
w iteracji dla bieżącej populacji, zawsze przesuwany jest ten sam, ustalony punkt. W DE/rand
przesuwany punkt jest za każdym razem losowany, przez co jest bardziej zmienny (ma większą wariancję).
Z tego powodu DE/mid oraz DE/best potrzebują większych współczynników skalujących niż DE/rand.

\begin{table}[H]
\centering
\begin{tabular}{ l | l }
algorytm         & współczynnik \\ \hline
DE/rand/k        & $\sqrt{\frac{2F^2}{2k}} = \frac{F}{\sqrt{k}}$ \\ 
DE/rand/$\infty$ & $\sqrt{2F^2} = \sqrt{2}F$ \\ \hline
DE/best/k        & $\sqrt{\frac{2F^2 + 1 - \frac{1}{\mu}}{2k}}$ \\
DE/best/$\infty$ & $\sqrt{2F^2 + 1 - \frac{1}{\mu}}$ \\ \hline
DE/mid/k         & $\sqrt{\frac{2F^2 + 1 - \frac{1}{\mu}}{2k}}$ \\
DE/mid/$\infty$  & $\sqrt{2F^2 + 1 - \frac{1}{\mu}}$ \\
\end{tabular}
\caption{Współczynniki skalujące dla algorytmów analizowanych w tej pracy}
\label{table:wspolczynniki}
\end{table}

\chapter{Metodyka testowania}
\label{chap:metodyka}

Twierdzenie No Free Lunch dla optymalizacji głosi, że nie istnieje najlepszy uniwersalny algorytm dla wszystkich zadań \cite{lunch}. 
Niezależnie od miary jakości algorytmu optymalizacyjnego, po uśrednieniu dla wszystkich zadań optymalizacyjnych, 
dowolne dwa różne algorytmy będą osiągały taką samą jakość wyników. Innymi słowy,
nie czyniąc żadnych założeń na temat natury optymalizowanej funkcji celu $f$, 
nigdy nie będziemy w stanie wykazać wyższości dowolnego algorytmu genetycznego nad np. błądzeniem losowym. 
Zatem należy przyjąć pewne założenia na temat optymalizowanych funkcji, żeby móc wskazać algorytm najlepszy (dla danego zbioru funkcji).
W idealnym przypadku, funkcje testowe odpowiadałaby tym spotykanym w~rzeczywistych zadaniach optymalizacji.
Nie wiadomo jednak jakie rodziny funkcji występują najczęściej w realnych problemach.
Dlatego w tej pracy przyjęto zbiór 7 funkcji wybranych z~zestawu BBOB 2013 \cite{noiseless}, szczegółowo opisanych w poniższym rozdziale.

\section{Zestaw funkcji testowych BBOB 2013}
\label{sec:zestaw}

Wszystkie funkcje z zestawu BBOB 2013 są zdefiniowane w całej przestrzeni $\mathbb{R}^D$, ale obszar
przeszukiwań został zawężony do $[-5; 5]^D$. Wszystkie poszukiwane minima leżą w tym obszarze.
Na rozwiązania spoza obszaru przeszukiwań nakładana jest zewnętrzna funkcja kary zdefiniowana w \cite{setup} nastepująco: 

$$ f_\text{pen} : \mathbb{R}^D \rightarrow \mathbb{R}, 
\textbf{x} \mapsto \sum\limits_{i=1}^D \max\left(0, |x_i| - 5\right)^2 $$ 
\\
Zestaw BBOB 2013 składa się z 54 funkcji, które dzielą się na dwie zasadnicze klasy:

\begin{itemize}
 \item[$\bullet$] 24 funkcje bez szumów o numerach od 1 do 24 \cite{noiseless}. 
 \item[$\bullet$] 30 funkcji z szumami o numerach od 101 do 130 \cite{noisy}.
\end{itemize} 

Funkcje bez szumów zwracają dokładną wartość funkcji celu w punkcie $x$. Funkcje z szumami wartość tą dodatkowo modyfikują używając 
losowego szumu. W tej pracy skoncentrowano się na funkcjach bez szumów. Można je podzielić ze względu
na ich właściwości:

\begin{itemize}
 \item[$\bullet$] Funkcje separowalne (numery od 1 do 5).
 \item[$\bullet$] Funkcje dobrze uwarunkowane numerycznie (od 6 do 9).
 \item[$\bullet$] Funkcje jednomodalne (z jednym ekstremum lokalnym), źle uwarunkowane numerycznie (od 10 do 14).
 \item[$\bullet$] Funkcje wielomodalne o regularnej strukturze (od 15 do 19).
 \item[$\bullet$] Funkcje wielomodalne o nieregularnej strukturze (od 20 do 24).
\end{itemize} 

Zgodnie z procedurą BBOB 2013 \cite{setup}, 
celem algorytmu optymalizacji jest wygenerowanie rozwiązania o wartości funkcji celu mniejszej bądź 
równej od $f_\text{opt} + 10^{-8}$, gdzie $f_\text{opt}$ to minimum optymalizowanej funkcji. 
W wielu przypadkach znalezienie takiego rozwiązania trwałoby zbyt długo.
Dlatego zastosowano kryterium stopu,
jakim jest ograniczenie na maksymalną liczbę wywołań funkcji oceny ($FEs$, ang. Function Evaluations), wynoszącą $10^5D$. 
Rozmiar populacji $\mu$ dla każdego algorytmu wynosił $10D$. 
Początkowa populacja była inicjalizowana zgodnie z rozkładem jednostajnym w obszarze przeszukiwań tzn.
w hipersześcianie $[-5; 5]^D$. Punkty, które w czasie optymalizacji wyjdą poza obszar
przeszukiwań, nie są naprawiane. Zewnętrzna funkcja kary
zapobiega znacznemu oddalaniu się punktów od interesującego obszaru.

Ze wzlędu na losowość procesu optymalizacji, na każdej funkcji algorytm jest uruchamiany 15 razy.
Z każdego uruchomienia zapisywany jest najlepszy wynik będący wartością funkcji dla najlepszego osobnika
pomniejszoną o $f_\text{opt}$.

Na funkcjach separowalnych, dobrze uwarunkowanych numerycznie lub jednomodalnych ewolucja różnicowa znajdowała rozwiązanie
bliskie optymalnemu na tyle często, że kłopotliwe było porównywanie jej wariantów. 
Różnice pomiędzy wariantami algorytmów udało się uchwycić przy użyciu trudniejszych funkcji.
Z tego powodu wybrano 7 funkcji 
wielomodalnych o numerach 15, 16, 19, 20, 21, 22, 24. Trzy z nich mają regularną strukturę (15, 16, 19), 
cztery nieregularną (20, 21, 22, 24). Wszystkie są niesymetryczne.
Poniżej przedstawiono je dokładniej.

\subsection{Funkcja numer 15 -- Rastrigina}

Ekstrema w oryginalnej funkcji Rastrigina są rozmieszczone regularnie i symetryczne. Dzięki dwóm transformacjom $T^\beta_\text{asy}$ 
oraz $T_\text{osz}$ zaburzono symetrię oraz regularność. Właściwości funkcji numer 15: 
\begin{itemize}
 \item[$\bullet$] Około $10D$ minimów lokalnych.
 \item[$\bullet$] Globalnie duże zmiany wartości, lokalnie - małe.
 \item[$\bullet$] Separowalna liniowo.
\end{itemize} 

$$ f_{15}(\bold{x}) = 10 \left( D - \sum^D_{i=1}\cos(2 \pi z_i) \right) + \norm{\bold{z}}^2 + f_\text{opt} $$
$$ \bold{z} = \bold{R} \bold{\Lambda}^{10} \bold{Q} T^{0.2}_\text{asy}(T_\text{osz}(\bold{R}(\bold{x} - \bold{x}^\text{opt}))) $$ 

$\bold{Q}$ i $\bold{R}$ to ortogonalne macierze obrotu.
$\bold{\Lambda}^\alpha$ to $D$-wymiarowa macierz diagonalna, w której $i$-ty element na przekątnej jest równy 
$\alpha^{\frac{1}{2}\frac{i-1}{D-1}}$ dla $i = 1, \dots, D$.

\begin{figure}[H]
\centering
\mbox{
\includegraphics[width=.45\textwidth]{img/15.png} \quad
\includegraphics[width=.45\textwidth]{img/15a.png} 
}
\caption{Dwuwymiarowa funkcja Rastrigina, strzałka wskazuje minimum \cite{noiseless}}
\end{figure}

\subsection{Funkcja numer 16 -- Weierstrassa}

Funkcja 16 jest obrócona w stosunku do oryginalnej funkcji Weierstrassa -- pierwszego opublikowanego przykładu
rzeczywistej funkcji ciągłej, nieróżniczkowalnej w żadnym punkcie \cite{szarek}.
Posiada powtarzalny, ale bardzo ,,wyboisty'' przebieg oraz więcej niż jedno optimum globalne. Właściwości:
\begin{itemize}
 \item[$\bullet$] Globalnie regularna, lokalnie nieregularna.
 \item[$\bullet$] Brak unikalnego optimum globalnego.
\end{itemize} 

$$ f_{16}(\bold{x}) = 10 \left( \frac{1}{D} \sum^D_{i=1} \sum^{11}_{k=0} 1 / 2^k \cos(2 \pi 3^k(z_i + 1/2)) - f_0 \right)^3 + \frac{10}{D} f_\text{pen}(\bold{x}) + f_\text{opt} $$
$$ \bold{z} = \bold{R} \bold{\Lambda}^{1/100} \bold{Q} T_\text{osz}(\bold{R}(\bold{x} - \bold{x}^\text{opt})) $$
$$ f_0 = \sum^{11}_{k=0} 1/2^k \cos(2 \pi 3^k 1/2) $$

\begin{figure}[H]
\centering
\mbox{
\includegraphics[width=.45\textwidth]{img/16.png} \quad
\includegraphics[width=.45\textwidth]{img/16a.png} 
}
\caption{Dwuwymiarowa funkcja Weierstrassa, strzałka wskazuje minimum \cite{noiseless}}
\end{figure}

\subsection{Funkcja numer 19 -- Griewanka-Rosenbrocka}

Funkcja 19 to złożenie funkcji Griewanka oraz Rosenbrocka, o ogromnej liczbie ekstremów lokalnych.
$$ f_{19}(\bold{x}) = \frac{10}{D - 1} \sum^{D-1}_{i=1}\left(\frac{s_i}{4000} - \cos(s_i)\right) + 10 + f_\text{opt} $$
$$ \bold{z} = \max\left(1, \frac{\sqrt{d}}{8}\right) \bold{Rx} + 0.5 $$
$$ \bigwedge_{i \in 1, \dots, D} s_i = 100(z^2_i - z_{i+1})^2 + (z_i - 1)^2 $$
$$ \bold{z}^\text{opt} = \bold{1} $$

\begin{figure}[H]
\centering
\mbox{
\includegraphics[width=.45\textwidth]{img/19.png} \quad
\includegraphics[width=.45\textwidth]{img/19a.png} 
}
\caption{Dwuwymiarowa funkcja numer 19, strzałka wskazuje minimum \cite{noiseless}}
\end{figure}

\subsection{Funkcja numer 20 -- Schwefela}

$\bold{1}^+_-$ oznacza wektor długości $D$ zawierający 1 oraz -1 losowane z równym prawdopodobieństwem.
W funkcji Schwefela najlepsze $2^D$ minimów lokalnych jest położonych stosunkowo blisko narożników atrakcyjnej hiperpłaszczyzny. Właściwości:
\begin{itemize}
 \item[$\bullet$] Częściowo separowalna.
 \item[$\bullet$] Charakterystyczna, obrócona struktura.
 \item[$\bullet$] Atrakcyjne obszary przeszukiwań znajdują się w narożnikach hiperpłaszczyzny.
\end{itemize} 

$$ f_{20}(\bold{x}) = -\frac{1}{D} \sum^D_{i=1} z_i \sin\left(\sqrt{|z_i|}\right) + 4.189828872724339 + 100 f_\text{pen}\left(\frac{\bold{z}}{100}\right) + f_\text{opt} $$
$$ \hat{\bold{x}} = 2 \cdot \bold{1}^+_- \otimes \bold{x} $$
$$ \bigwedge_{i \in 1, \dots, D} \hat{z}_1 = \hat{x}_1, \hat{z}_{i+1} = \hat{x}_{i+1} + 0.25(\hat{x}_i - x_i^\text{opt}) $$ 
$$ \bold{x}^\text{opt} = \frac{4.2096874633}{2} \bold{1}^+_- $$

\begin{figure}[H]
\centering
\mbox{
\includegraphics[width=.45\textwidth]{img/20.png} \quad
\includegraphics[width=.45\textwidth]{img/20a.png} 
}
\caption{Dwuwymiarowa funkcja Schwefela, strzałka wskazuje minimum \cite{noiseless}}
\end{figure}

\subsection{Funkcja numer 21 -- górki gaussowskie Gallaghera 101-me}

Funkcja posiada 101 ekstremów lokalnych, których położenie oraz wielkość są losowe i~niezależne od siebie.
Analizowanie wyników na tej funkcji pomaga odpowiedź na następujące pytanie: Czy przeszukiwanie jest efektywne, gdy funkcja celu
nie ma żadnej globalnej struktury?
$$ f_{21}(\bold{x}) = T_\text{osz}\left(10-\max_{i=1}^{101}w_i\exp\left(-\frac{1}{2D}(\bold{x}-\bold{y}_i)^T\bold{R}^T\bold{C}_i\bold{R}(\bold{x}-\bold{y}_i)\right)\right)^2 + f_\text{pen}(\bold{x}) + f_\text{opt} $$
\[
w_i =
\begin{cases} 
1.1 + 8 \cdot \frac{i-2}{99} & \mbox{dla } i = 2, \dots, 101 \\ 
10 & \mbox{dla } i = 1
\end{cases}
\] 
$\bold{C}_i$ jest zdefiniowane w \cite{noiseless}. 

\begin{figure}[H]
\centering
\mbox{
\includegraphics[width=.45\textwidth]{img/21.png} \quad
\includegraphics[width=.45\textwidth]{img/21a.png} 
}
\caption{Dwuwymiarowa funkcja numer 21, strzałka wskazuje minimum \cite{noiseless}}
\end{figure}

\subsection{Funkcja numer 22 -- górki gaussowskie Gallaghera 21-hi}

Funkcja posiada 21 ekstremów lokalnych, których położenie oraz wielkość są losowe i~niezależne od siebie.
Analizowanie wyników na tej funkcji pomaga odpowiedź na następujące pytanie: W~porównaniu do funkcji 21, jak 
wysoki wskaźnik uwarunkowania wpływa na efektywność przeszukiwania?
$$ f_{22}(\bold{x}) = T_\text{osz}\left(10-\max_{i=1}^{21}w_i\exp\left(-\frac{1}{2D}(\bold{x}-\bold{y}_i)^T\bold{R}^T\bold{C}_i\bold{R}(\bold{x}-\bold{y}_i)\right)\right)^2 + f_\text{pen}(\bold{x}) + f_\text{opt} $$
\[
w_i =
\begin{cases} 
1.1 + 8 \cdot \frac{i-2}{19} & \mbox{dla } i = 2, \dots, 21 \\ 
10 & \mbox{dla } i = 1
\end{cases}
\] 
$\bold{C}_i$ jest zdefiniowane w \cite{noiseless}. 

\begin{figure}[H]
\centering
\mbox{
\includegraphics[width=.45\textwidth]{img/22.png} \quad
\includegraphics[width=.45\textwidth]{img/22a.png} 
}
\caption{Dwuwymiarowa funkcja numer 22, strzałka wskazuje minimum \cite{noiseless}}
\end{figure}

\subsection{Funkcja numer 24 -- Lunačka}

Funkcja Lunačka jest również nazywana podwójnym Rastriginem (bi-Rastrigin). Funkcja ta ma dużą liczbę ekstremów lokalnych. 
Na jej dwuwymiarowym wykresie można zauważyć dwa charekterystyczne zbiory przyciągania -- ,,górki''. Na wykresach po lewej stronie
wartości funkcji $f$ maleją ku górze, żeby minima globalne były dobrze widoczne.
Żeby znaleźć optimum, algorytm optymalizacji musi najpierw poprawnie wybrać ,,górkę'',
a następnie dokładnie przeszukać wielomodalny obszar wewnątrz jej. Funkcja została skonstruowana w taki sposób, aby zmylić
niektóre algorytmy ewolucyjne z dużym rozmiarem populacji. Obszar przyciągania zawierający zwodnicze
minimum lokalne stanowi około 70\% całej przestrzeni przeszukiwań.
Analiza wyników na tej funkcji pomaga odpowiedzieć na następujące pytanie:
Czy przeszukiwanie może mieć charakter lokalny w skali globalnej oraz charakter globalny w skali lokalnej?
$$ f_{24}(\bold{x}) = \min \left( \sum^D_{i=1}(\hat{x}_i - \mu_0)^2, dD + s \sum^D_{i=1}(\hat{x}_i - \mu_1)^2 \right) + 10 \left( D - \sum^D_{i=1} \cos(2 \pi z_i) \right) + 10^4 f_\text{pen}(\bold{x}) $$
$$ \hat{\bold{x}} = 2 \text{ sign}(\bold{x}^{\text{opt}}) \otimes \bold{x}, \bold{x}^{\text{opt}} = \mu_0 \bold{1}_-^+ $$
$$ \bold{z} = \bold{Q}\Lambda^{100}\bold{R}(\hat{\bold{x}} - \mu_0\bold{1}) $$
$$ \mu_0 = 2.5, \mu_1 = -\sqrt{\frac{\mu_0^2-d}{s}}, s = 1 - \frac{1}{2\sqrt{D+20}-8.2}, d=1 $$

\begin{figure}[H]
\centering
\mbox{
\includegraphics[width=.45\textwidth]{img/24.png} \quad
\includegraphics[width=.45\textwidth]{img/24a.png} 
}
\caption{Dwuwymiarowa funkcja Lunačka, strzałka wskazuje minimum \cite{noiseless}}
\end{figure}

\section{Porównywanie wyników}

Duża liczba wariantów algorytmów wymaga systematycznego i wiarygodnego sposobu porównywania
i prezentacji wyników. Testy istotności statystycznej pozwalają w szybki sposób rozstrzygnąć czy 
wyniki osiągane przez dwa algorytmy są od siebie istotnie różne. Dystrybuanty empiryczne
pozwalają zorientować się na ile wyniki algorytmów różnią się od siebie. Procent osobników, które
znalazły się poza przestrzenią przeszukiwań, pozwala ocenić czy algorytm dotarł do krawędzi
obszaru przeszukiwań czy pozostawał cały czas w jego wnętrzu.

\subsection{Testy istotności statystycznej}
\label{section:testy_istotnosci}

W tej pracy jako podstawowy sposób porównywania wyników został wykorzystany test 
Wilcoxona dla par obserwacji. Zaproponowany pierwotnie jako test przesunięcia dla dwóch równolicznych 
próbek przez Franka Wilcoxona w 1945, uogólniony następnie przez Manna i Whitneya w 1947
dla przypadku różnolicznych próbek \cite{mann}. Test Wilcoxona to nieparametryczna alternatywa dla testu t-Studenta w 
przypadku dwóch równolicznych próbek. Test t-Studenta sprawdza hipotezę zerową o równości
średnich arytmetycznych w odpowiadających im populacjach, natomiast test Wilcoxona
weryfikuje równość median. Średnia jest wrażliwa na wartości odstające,
natomiast mediana nie. Ponadto, test t-Studenta jest parametryczny, to znaczy zakłada pewien rozkład
wartości z badanej próby. Test Wilcoxona jest nieparametryczny, to znaczy,
nie zakłada nic na temat rozkładu badanych wartości i dlatego został wybrany.

$n$ par obserwacji pochodzi z dwóch zbiorów. Pierwszy element pary pochodzi ze zbioru pierwszego, drugi element pary 
ze zbioru drugiego. W tej pracy $n=15$,
ponieważ liczba niezależnych uruchomień algorytmu wynosiła 15. Pierwszy zbiór odpowiada algorytmowi A i zawiera
liczby $x_1, x_2, \dots, x_{n}$, drugi -- algorytmowi B i zawiera liczby $y_1, y_2, \dots, y_{n}$. 
Liczby w zbiorach oznaczają różnice wartości funkcji celu najlepszego punktu oraz minimum globalnego
optymalizowanej funkcji. Zostały spełnione wszystkie założenia dla testu Wilcoxona:

\begin{enumerate}
 \item Wartości $x_i$ i $y_i$ są parowane w sposób losowo niezależny od siebie. 
 \item Wartości $x_i$ i $y_i$ pochodzą z populacji o rozkładzie ciągłym.
 \item Wartości $x_i$ i $y_i$ można porównywać ze sobą, jednoznacznie stwierdzając która jest większa, mniejsza,
bądź równa.
\end{enumerate}

Testowana hipoteza zerowa $H_0$ brzmi: ,,różnica pomiędzy medianami rozkładów generujących zbiory A i B wynosi zero''.
Hipoteza alternatywna $H_1$ brzmi: ,,różnica pomiędzy medianami rozkładów generujących zbiory A i B jest różna od zera''.
Test Wilcoxona przebiega następująco:

\begin{enumerate}
 \item Obliczenie różnic $d_i = y_i - x_i$.
 \item Usunięcie par dla których $d_i = 0$. Niech $N \leq n$ oznacza liczbę pozostałych par.
 \item Posortowanie rosnąco wartości bezwględnych różnic $|d_i|$.
 \item Rangowanie posortowanego zbioru rangami $R_i$, poczynając od rangi równej 1. 
 \item Obliczenie statystyki $W = |\sum\limits_{i=1}^{N} \sgn(d_i)R_i|$.
 \item Rozkład $W$ zbiega do rozkładu normalnego wraz ze wzrostem $N$. Obliczane jest
 $p = \frac{W - 0.5}{\sigma_W}$, gdzie $\sigma_W = \sqrt{\frac{N(N+1)(2N+1)}{6}}.$ 
 \item Jeśli $z > z_{critical}$, hipoteza $H_0$ jest odrzucana, w przeciwnym przypadku -- przyjmowana. 
Wartości $z_{critical}$ zależą od przyjętego poziomu ufności. 
W tej pracy poziom ufności wynosił $0.05$, co odpowiada $z_{critical}=1.96$ \cite{lowry}.  
\end{enumerate}

Jeśli test nie pozwalał na odrzucenie hipotezy zerowej, uznawano, że wyniki porównywanych algorytmów
nie są istotnie różne od siebie. 
Jeśli test odrzucał hipotezę zerową, uznawano, że algorytmy A i B dają różne wyniki.
Wówczas porówywane były mediany obu zbiorów. Algorytm, którego mediana wyników była niższa,
uznawany był za lepszy.

\subsection{Dystrybuanty empiryczne}

W celu porównania wyników osiąganych przez algorytmy, 
wykreślono dystrybuanty empiryczne najlepszych wyników z każdego uruchomienia 
dla wszystkich algorytmów na jednej funkcji. Oś x wykresu
odpowiada wartościom najlepszych wyników osiąganych przez algorytmy, tzn. różnicom wartości funkcji celu najlepszego
punktu oraz minimum globalnego optymalizowanej funkcji. Oś y odpowiada natomiast 
estymowanemu prawdopodobieństwu, z jakim algorytm osiąga dany wynik. Dystrybuanty pozwalają również
określić jak bliskie minimum było najlepsze znalezione przez algorytm rozwiązanie.

\subsection{Procent osobników poza obszarem przeszukiwań}

W każdym uruchomieniu zliczana była liczba osobników, których jakakolwiek współrzędna wykraczała poza
obszar przeszukiwań zdefiniowany jako $[-5; 5]^D$. Następnie obliczany był stosunek liczby 
osobników spoza obszaru przeszukiwań do liczby wszystkich wygenerowanych osobników. Jeśli algorytm
nie znajdował minimum, wówczas liczba wszystkich generowanych osobników była równa maksymalnej liczbie
populacji razy $\mu$, czyli $\frac{FEs}{\mu} \cdot \mu = FEs = 10^5D$. 
Dla jednego algorytmu i jednej funkcji, procent osobników
poza obszarem przeszukiwań uśredniano z 15 niezależnych uruchomień.



\chapter{Wyniki eksperymentów}

\section{Parametry eksperymentów}

Dla funkcji o małej liczbie wymiarów trudno dostrzec różnicę pomiędzy algorytmami. Im wyższy wymiar,
tym trudniejszy problem i dłuższy czas obliczeń, ale różnice pomiędzy algorytmami stają się bardziej 
widoczne. Dlatego liczba wymiarów $D$ w testach wynosiła 10, 20, 40 oraz 80. 

Współczynnik skalujący algorytmu DE/rand/1 ($F$) wynosił $0.9$. 
Dla pozostałych algorytmów współczynnik skalujący był obliczany zgodnie z tabelą
\ref{table:wspolczynniki}.
W każdym z algorytmów zostało wykorzystane krzyżowanie wymieniające (DE/*/k/bin) z
prawdopodobieństwem krzyżowania $CR = 0.9$ \cite{ronkkonen}.

\section{Implementacja}

Zestaw BBOB 2013 dostarcza implementację wszystkich funkcji testowych w języku C. Funkcje te obliczane
są miliardy razy podczas eksperymentów, dlatego ważne jest, żeby wykonywały się jak najszybciej.
Procedura testująca oraz badane algorytmy zostały zaimplementowane w 
Javie. Procedura testująca wywołuje funkcje napisane w~C~dzięki Java Native Interface (JNI).
Do automatyzacji testów oraz przetwarzania wyników zostały napisane skrypty w sh 
(języku powłoki Bourne'a) oraz języku~R. Do przeprowadzenia testów istotności statystycznej Wilcoxona 
użyto funkcji \texttt{wilcox.test} z pakietu \texttt{stats} języka R. Dystrybuanty empiryczne
zostały obliczone dzięki funkcji \texttt{ecdf} (ang. empirical cumulative distribution function),
pochodzącej z tego samego pakietu.

Poprawność implementacji algorytmów w Javie została zweryfikowana dzięki testom macierzy kowariancji
populacji. Zbadano wartości macierzy po pierwszej mutacji, uśrednione z~10000 niezależnych uruchomień. 
Średnie wartości macierzy kowariancji były takie same dla wszystkich badanych algorytmów, zgodnie z 
założeniami teoretycznymi. Uśrednioną macierz kowariancji populacji przedstawia tabela 
\ref{table:cov_matrix}. 

\begin{table}[H]
\centering
\begin{tabular}{ c c c c c c c c c c }
21.36 & 0.02 & 0.01 & -0.04 & 0.06 & -0.01 & -0.03 & -0.02 & -0.01 & 0.03 \\
0.02 & 21.37 & 0.00 & 0.01 & -0.01 & 0.02 & 0.03 & 0.02 & 0.03 & 0.00 \\
0.01 & 0.00 & 21.40 & -0.01 & -0.03 & -0.03 & -0.09 & 0.04 & 0.01 & -0.02 \\
-0.04 & 0.01 & -0.01 & 21.37 & 0.02 & 0.00 & 0.02 & 0.07 & 0.03 & -0.03 \\
0.06 & -0.01 & -0.03 & 0.02 & 21.43 & -0.00 & 0.02 & 0.01 & -0.01 & 0.01 \\
-0.01 & 0.02 & -0.03 & 0.00 & -0.00 & 21.41 & -0.00 & -0.01 & 0.02 & -0.01 \\
-0.03 & 0.03 & -0.09 & 0.02 & 0.02 & -0.00 & 21.38 & 0.04 & 0.05 & -0.01 \\
-0.02 & 0.02 & 0.04 & 0.07 & 0.01 & -0.01 & 0.04 & 21.40 & -0.05 & 0.04 \\
-0.01 & 0.03 & 0.01 & 0.03 & -0.01 &  0.02 & 0.05 & -0.05 & 21.39 & 0.02 \\
0.03 & 0.00 & -0.02 & -0.03 & 0.01 & -0.01 & -0.01 &  0.04 & 0.02 & 21.37 \\
\end{tabular}
\caption{Uśredniona macierz kowariancji dla 10-wymiarowej funkcji numer 15.}
\label{table:cov_matrix}
\end{table}

Wszystkie funkcje użyte do testów są nieseparowalne liniowo.
Dlatego wartości macierzy kowariancji populacji poza główną przekątną są bliskie 0. Oznacza to,
iż korelacja liniowa Pearsona pomiędzy wartościami różnych cech praktycznie nie występuje, wymiary
są niezależne od siebie. Z kolei wartość bliska 21.4 powtarzająca się na głównej przekątnej 
oznacza taką samą wariancję wartości w każdym z wymiarów, tzn. dla każdej cechy osobnika.
Przedstawiona macierz kowariancji jest identyczna dla wszystkich algorytmów, 
dzięki odpowiednim współczynnikom skalującym
$F$ wyprowadzonych wcześniej w tej pracy. Dzięki takiej samej macierzy kowariancji algorytmy
mają takie same warunki i wyniki przez nie zwracane zależą tylko od sposobu mutacji, nie od jej
zasięgu. 
Wartość bliską 21.4 można uzasadnić dzięki wyprowadzonej zależności \eqref{eq:macierz_kow_mutanta},
wiążącej macierz kowariancji mutanta $\C[u_i]$
z macierzą kowariancji początkowej populacji $\C[P]$. Populacja początkowa była losowana zgodnie z 
rozkładem jednostajnym na przedziale $[-5;5]^{10}$, zatem na głównej przekątnej $\C[P]$
znajduje się liczba $\frac{10^2}{12} \approx 8.3$, zgodnie ze wzorem na wariancję rozkładu jednostajnego 
$\frac{(b - a)^2}{12}$. $F$ było równe 0.9, zatem 
$\C[u_i] = \C[P](2 \cdot 0.9^2 + 1) = \C[P] \cdot 2.62 \approx \bold{\Lambda_{10}} \cdot 8.3 \cdot 2.62 \approx \bold{\Lambda_{10}} \cdot 21.7$, gdzie $\bold{\Lambda_{10}}$
to 10-wymiarowa macierz diagonalna. 

\section{Wnioski}

Biorąc pod uwagę najwyższy zbadany wymiar i wszystkie 7 funkcji testowych,
końcowy ranking efektywności algorytmów przedstawia się 
nastepująco:

\begin{enumerate}
 \item DE/mid/1
 \item DE/rand/1
 \item DE/best/1
 \item DE/mid/2
 \item DE/rand/2
 \item DE/best/2
 \item DE/mid/6, DE/mid/$\infty$, DE/rand/6, DE/rand/$\infty$, DE/best/6, DE/best/$\infty$
\end{enumerate}

Istnieje jednak nieskończenie wiele funkcji na których DE/mid/1 nie jest najlepszym wyborem, np. rodzina
funkcji podobnych do funkcji numer 16 -- Weierstrassa. Na takich funkcjach lepiej sprawuje się 
DE/best/1. Gdyby zbadać wszystkie możliwe funkcje (co nie jest możliwe, jest ich nieskończenie wiele),
okazałoby się, że żadan algorytm nie jest
najlepszy. Mówi o tym twierdzenie No Free Lunch, opisane w rozdziale \ref{chap:metodyka}.

W notacji stosowanej w tabelach brak statystycznej różnicy oznaczono znakiem ,,$\cdotp$''.
Jeśli test istotności (opisany w rozdziale \ref{section:testy_istotnosci})
odrzucał hipotezę zerową, uznawano, że algorytmy A i B dają różne wyniki.
Wówczas porówywane były mediany obu zbiorów. Algorytm, którego mediana wyników była niższa,
uznawany był za lepszy i oznaczany znakiem ,,+''. Algorytm istotnie gorszy był oznaczany znakiem ,,--''.

\subsection{Wpływ liczby wymiarów na jakość rozwiązań}

Wraz ze wzrostem liczby wymiarów, algorytm DE/mid/1 sprawuje się coraz lepiej w porównaniu do
pozostałych algorytmów.
W~80~wymiarach jednoznacznie wygrywa ze wszystkimi pozostałymi algorytmami na 6~z~7~funkcji testowych.
Jedynie na funkcji numer 16 przegrywa z DE/best/1 oraz remisuje z~DE/rand/1.
W~40~wymiarach prowadzenie DE/mid/1 wygląda podobnie, jedynie różnica w~stosunku do DE/rand/1
jest mniejsza, z którym to wygrywa na 3 funkcjach, remisuje również na 3, a~na 1~przegrywa.
W~20~wymiarach zajmuje drugie miejsce, ustępując DE/rand/1.
W~10~wymiarach było najwięcej remisów, DE/mid/1 był niepokonany na funkcjach numer 15, 19 oraz 24.

\subsection{Wpływ optymalizowanych funkcji}

Na funkcji 15 oraz 19 zawsze zwyciężał DE/mid/1 -- poza jednym remisem z DE/rand/1 w 20 wymiarach.
\% osobników poza obszarem przeszukiwań na tych funkcjach dla DE/mid/1 zawsze był najniższy.

Funkcja 16 była najgorsza dla DE/mid/1. W 10 i 20 wymiarach, zawsze wygrywał na niej DE/rand/1,
zaś w 40 i 80 wymiarach -- DE/best/1. Spośród najlepszej trójki algorytmów (DE/mid/1, DE/best/1,
DE/rand/1) zawsze najniższy \% osobników poza obszarem przeszukiwań miał DE/best/1, 
potem DE/rand/1, na końcu DE/mid/1. 

Na funkcji 20 w wymiarach niższych niż 80 zawsze wygrywał DE/rand/1. Dopiero w 80 wymiarze DE/mid/1
objął prowadzenie. DE/mid/1 zawsze miał najniższy \% osobników poza obszarem przesukiwań.

Na funkcji 21 w niższych wymiarach najlepsze wyniki dawał DE/rand/1. W 40 wymiarach wygrał ze 
wszystkimi algorytmami, oprócz DE/mid/1, z którym zremisował. W 80 wymiarach jednak DE/mid/1
okazał się bezkonkurencyjny. DE/mid/1 miał zawsze niższy \% osobników poza obszarem przesukiwań
od DE/rand/1. Mimo, że funkcja 21 nie ma żadnej globalnej struktury, dla DE/mid/1 nie stanowi to 
przeszkody.

Na funkcji 22 w niższych wymiarach sytuacja była remisowa. 
W 40 i 80 wymiarach relacja pomiędzy DE/mid/1 oraz DE/rand/1 jest taka sama jak w przypadku funkcji 21
-- w 40 wymiarach remis, natomiast w 80 zwycięstwo DE/mid/1. \% osobników DE/mid/1 poza obszarem 
przesukiwań cały czas utrzymuje się na niskim poziomie, niemal zawsze najniższym w porównaniu do 
innych algorytmów.
W porównaniu do funkcji 21, wysoki wskaźnik uwarunkowania powoduje mały spadek efektywności
przeszukiwania testowanych algorytmów. Ich kolejność w rankingu jakości zwracanych rozwiązań nie 
zmienia się.

Na funkcji 24 DE/mid/1 przegrał tylko w wymiarze 20 z DE/rand/1, wszystkie pozostałe porównania 
wygrał. To pokazuje, że przeszukiwanie w DE/mid/1 może mieć charakter lokalny w skali globalnej 
oraz charakter globalny w skali lokalnej. Wszystkie algorytmy na funkcji 24 miały niski
\% osobników poza obszarem przesukiwań, bliski zeru. 

\subsection{Wpływ liczby wymiarów na liczbę osobników poza obszarem przeszukiwań}

Wraz ze wzrostem liczby wymiarów, początkowo odsetek osobników poza obszarem przeszukiwań zazwyczaj
również wzrastał. W wyższych wymiarach jednak nie jest to zasadą. 
Wraz z dalszym wzrostem liczby wymiarów,
liczba osobników poza obszarem przeszukiwań w~licznych przypadkach zaczęła spadać.
To zjawisko jest związane z problemem ,,wchodzenia w narożniki'', który zaczyna się coraz bardziej
nasilać, im wyższy wymiar. Problem polega na tym, że im wyższy wymiar, tym prawdopodobieństwo
wylosowania punktu blisko każdego z~ograniczeń, jest coraz niższe. Przykładowo, w dwóch wymiarach,
niech definicją ,,blisko każdego z ograniczeń'' będzie pole pozostałe po wycięciu 
okręgu wpisanego w kwadrat. W tym obszarze może znajdować się poszukiwane optimum globalne.
Stosunek pola przy narożnikach do całego kwadratu wynosi $\frac{\pi r^2}{(2r)^2} \approx 0.79$.
W trzech wymiarach stosunek ten spada do $\frac{\frac{4}{3}\pi r^3}{(2r)^3} \approx 0.52$.
W dziesięciu -- do około 0.08.
Przy liczbie wymiarów dążacej do nieskończności, stosunek ten spada do 0.

\subsection{Wpływ liczby osobników poza obszarem przeszukiwań na jakość rozwiązań}

Im algorytm miał mniejszą liczbę osobników poza obszarem przeszukiwania, tym lepsze osiągał wyniki.
DE/mid/1 na 6 funkcjach, na których zwyciężył, zawsze miał najmniej osobników poza obszarem
przeszukiwania. W przypadku funkcji 16 -- jedynej na której przegrał -- nie miał. Najmniej
wówczas miał DE/best/1 i to właśnie ten wariant zwyciężył w wysokich wymiarach. Istnieje zatem silna korelacja
pomiędzy jakością rozwiązań a zdolnością algorytmu do utrzymywania się w obszarze przeszukiwań.
W pracy \cite{boundary} porównano różne sposoby naprawiania rozwiązań i również zauważono, że im mniej
osobników jest naprawianych, tym algorytm otrzymuje lepsze wyniki.

W tej pracy, liczba osobników poza obszarem przeszukiwania dochodziła do 47\%, na funkcji 16. To pokazuje,
jak ważny jest sposób radzenia sobie z takimi punktami. W eksperymentach nie 
naprawiano osobników, stosowano natomiast zewnętrzną funkcję kary opisaną w rodziale \ref{sec:zestaw},
zgodnie z wytycznymi BBOB 2013. Wyniki na poziomie 47\% pokazują, że funkcja kary nie jest 
skuteczną metodą. Prawdopodobnie efektywność wszystkich algorytmów można znacząco poprawić 
stosując sposoby naprawiania punktów opisane w \cite{boundary}, np. ponowne próbkowanie (resampling).

\section{Porównanie z algorytmami biorącymi udział w konkursie BBOB 2013}

W tabeli \ref{table:bbob2013mid20} porównano DE/mid/1 z algorytmami startującymi w konkursie BBOB 2013
w 20 wymiarach.
Wyniki dla 20 wymiarów i niższych dostarczyli wszyscy członkowie konkursu. 
Dla 40 wymiarów -- nieliczni, ponieważ było to opcjonalne.
Dla 80 wymiarów -- nikt, ponieważ nie było to wymaganiem.
DE/mid/1 okazał się gorszy od 17 z 26 algorytmów z BBOB 2013 dla 20 wymiarów. DE/mid/1 w małej liczbie wymiarów
jeszcze ,,nie rozwija skrzydeł'' i nawet DE/rand/1 jest od niego lepszy. Dlatego do algorytmów z BBOB 2013 porównano również
DE/rand/1 -- okazał się gorszy od 9 algorytmów.

\begin{table}[H]
\centering
\begin{tabular}{ | l | c | c | c | c | c | c | c | c | }
\hline		 & \multicolumn{7}{c|}{Numer funkcji testowej} & \\  \hline
Algorytm         &15& 16& 19& 20& 21& 22& 24 & Suma \\  \hline
BIPOP-aCMA-STEP	 & -- & -- & -- & -- & -- & -- & -- & -7 \\ 
BIPOP-saACM-k	 & -- & -- & -- & -- & -- & -- & -- & -7 \\
CGA-grid100	 & + & -- & + & -- & + & + & + & 3 \\
CGA-grid16	 & + & -- & + & -- & + & + & + & 3 \\ 
CMAES\_Hutter	 & + & -- & + & -- & + & $\cdot$ & + & 2 \\
DE\_Pal	 & + & $\cdot$ & + & -- & $\cdot$ & $\cdot$ & + & 2\\
fmincon	 & + & -- & -- & -- & -- & -- & + & -3 \\
fminunc	 & + & + & -- & -- & -- & -- & + & -1 \\
GA-100	 & $\cdot$ & -- & $\cdot$ & -- & + & $\cdot$ & -- & -2 \\
HCMA	 & -- & -- & -- & -- & -- & -- & -- & -7 \\
HILL	 & + & -- & + & -- & + & $\cdot$ & + & 2\\
HMLSL	 & $\cdot$ & -- & -- & -- & $\cdot$ & $\cdot$ & -- & -4 \\
IP	 & -- & -- & -- & -- & $\cdot$ & $\cdot$ & -- & -5 \\
IP-10DDr	 & -- & -- & -- & -- & -- & $\cdot$ & -- & -6 \\
IP-500	 & -- & -- & -- & -- & $\cdot$ & $\cdot$ & -- & -5 \\
MEMPSODE	 & $\cdot$ & -- & -- & -- & -- & -- & -- & -6 \\
MLSL	 & + & -- & -- & -- & -- & -- & + & -3 \\
OQNLP	 & + & -- & -- & -- & $\cdot$ & $\cdot$ & -- & -3 \\
P-DCN	 & + & -- & + & -- & + & $\cdot$ & + & 2 \\
P-zero	 & + & -- & + & -- & + & $\cdot$ & + & 2\\
PRCGA	 & $\cdot$ & -- & -- & -- & + & $\cdot$ & -- & -3 \\
Simplex	 & + & -- & -- & -- & $\cdot$ & $\cdot$ & + & -1\\
tany	 & -- & -- & -- & -- & -- & $\cdot$ & -- & -6 \\
texp	 & -- & -- & -- & -- & -- & -- & -- & -7 \\
U-DCN	 & + & -- & $\cdot$ & -- & + & + & + & 2\\
U-zero	 & + & -- & $\cdot$ & -- & + & + & + & 2\\ \hline
Suma	 & 6 & -23 & -9 & -26 & 0 & -4 & 0 & -56 \\ \hline

\end{tabular}
\caption{Porównanie DE/mid/1 do 26 algorytmów z BBOB 2013 w 20 wymiarach}
\label{table:bbob2013mid20}
\end{table}

\begin{table}[H]
\centering
\begin{tabular}{ | l | c | c | c | c | c | c | c | c | }
\hline		 & \multicolumn{7}{c|}{Numer funkcji testowej} & \\  \hline
Algorytm         &15& 16& 19& 20& 21& 22& 24 & Suma \\  \hline
BIPOP-aCMA-STEP	 & -- & -- & -- & -- & -- & -- & -- & -7\\
BIPOP-saACM-k	 & -- & -- & -- & -- & -- & -- & -- & -7\\
CGA-grid100	 & + & + & + & + & + & + & + & 7\\
CGA-grid16	 & + & + & $\cdot$ & + & + & + & + & 6\\
CMAES\_Hutter	 & + & + & + & + & + & $\cdot$ & + & 6\\
DE\_Pal	 & + & + & + & $\cdot$ & $\cdot$ & $\cdot$ & + & 4\\
fmincon	 & + & + & -- & + & -- & -- & + & 1\\
fminunc	 & + & + & -- & + & -- & -- & + & 1\\
GA-100	 & + & + & -- & + & + & $\cdot$ & $\cdot$ & 3\\
HCMA	 & -- & -- & -- & -- & -- & -- & -- & -7 \\
HILL	 & + & + & $\cdot$ & + & + & $\cdot$ & + & 5\\
HMLSL	 & + & + & -- & $\cdot$ & $\cdot$ & $\cdot$ & $\cdot$ & 1\\
IP	 & -- & -- & -- & -- & $\cdot$ & $\cdot$ & -- & -5 \\
IP-10DDr	 & -- & -- & -- & -- & -- & $\cdot$ & -- & -6\\
IP-500	 & -- & -- & -- & + & $\cdot$ & $\cdot$ & -- & -3\\
MEMPSODE	 & -- & $\cdot$ & -- & $\cdot$ & -- & -- & -- & -5\\
MLSL	 & + & + & -- & + & -- & -- & + & 1\\
OQNLP	 & + & + & -- & + & $\cdot$ & $\cdot$ & $\cdot$ & 2\\
P-DCN	 & + & + & + & + & + & + & + & 7\\
P-zero	 & + & + & + & + & + & $\cdot$ & + & 6\\
PRCGA	 & + & + & -- & + & + & $\cdot$ & -- & 2\\
Simplex	 & + & + & -- & + & $\cdot$ & $\cdot$ & + & 3\\
tany	 & -- & -- & -- & -- & -- & $\cdot$ & -- & -6\\
texp	 & -- & -- & -- & $\cdot$ & -- & -- & -- & -6\\
U-DCN	 & + & + & -- & + & + & + & + & 5 \\
U-zero	 & + & + & $\cdot$ & + & + & + & + & 6\\ \hline
Suma     & 8 & 9 & -13 & 10 & 0 & -3 & 3 & 14 \\ \hline
\end{tabular}
\caption{Porównanie DE/rand/1 do 26 algorytmów z BBOB 2013}
\end{table}

\begin{table}[H]
\centering
\begin{tabular}{ | l | c | c | c | c | c | c | c | c | }
\hline		 & \multicolumn{7}{c|}{Numer funkcji testowej} & \\  \hline
Algorytm         &15& 16& 19& 20& 21& 22& 24 & Suma \\  \hline
IPOP-aCMA-STEP	 & -- & -- & -- & -- & -- & -- & -- & -7\\
BIPOP-saACM-k	 & -- & -- & -- & -- & -- & -- & -- & -7\\
CGA-grid100	 & + & -- & + & -- & + & $\cdot$ & + & 2 \\
CGA-grid16	 & + & -- & + & -- & $\cdot$ & $\cdot$ & + & 1 \\
CMAES	 & $\cdot$ & -- & + & -- & + & $\cdot$ & + & 1  \\
GA-100	 & $\cdot$ & -- & -- & -- & $\cdot$ & $\cdot$ & -- & -4 \\
HILL	 & + & -- & + & -- & $\cdot$ & $\cdot$ & + & 1\\
IP	 & -- & -- & -- & -- & $\cdot$ & -- & -- & -6 \\
IP-10Dr	 & -- & -- & -- & -- & $\cdot$ & -- & -- & -6 \\
IP-500	 & -- & -- & -- & -- & $\cdot$ & $\cdot$ & -- & -5 \\
MEMPSODE	 & $\cdot$ & -- & -- & -- & -- & -- & -- & -6 \\
PRCGA	 & $\cdot$ & -- & -- & -- & $\cdot$ & $\cdot$ & -- & -4 \\
tany	 & -- & -- & -- & -- & -- & -- & -- & -7 \\
texp	 & -- & -- & -- & -- & -- & -- & -- & -7 \\ \hline
Suma	 & -4 & -14 & -6 & -14& -3 & -7 & -6 & -54 \\ \hline
\end{tabular}
\caption{Porównanie DE/mid/1 do 14 algorytmów z BBOB 2013 w 40 wymiarach}
\end{table}

\begin{table}[H]
\centering
\begin{tabular}{ | l | c | c | c | c | c | c | c | c | }
\hline		 & \multicolumn{7}{c|}{Numer funkcji testowej} & \\  \hline
Algorytm         &15& 16& 19& 20& 21& 22& 24 & Suma \\  \hline
BIPOP-aCMA-STEP	 & -- & -- & -- & -- & -- & -- & -- & -7 \\
BIPOP-saACM-k	 & -- & -- & -- & -- & -- & $\cdot$ & -- & -6 \\
CGA-grid100	 & + & -- & + & -- & + & $\cdot$ & + & 2 \\
CGA-grid16	 & + & -- & + & -- & $\cdot$ & + & + & 2\\
CMAES	 & $\cdot$ & -- & + & -- & + & $\cdot$ & + & 1 \\
GA-100	 & -- & -- & -- & -- & $\cdot$ & $\cdot$ & -- & -5 \\
HILL	 & + & -- & + & -- & $\cdot$ & + & + & 2 \\
IP	 & -- & -- & -- & -- & $\cdot$ & $\cdot$ & -- & -5 \\
IP-10Dr	 & -- & -- & -- & -- & -- & $\cdot$ & -- & -6 \\
IP-500	 & -- & -- & -- & -- & $\cdot$ & $\cdot$ & -- & -5\\
MEMPSODE	 & -- & -- & -- & -- & -- & -- & -- & -7 \\
PRCGA	 & -- & -- & -- & -- & $\cdot$ & $\cdot$ & -- & -5\\
tany	 & -- & -- & -- & -- & -- & $\cdot$ & -- & -6 \\
texp	 & -- & -- & -- & -- & -- & $\cdot$ & -- & -6 \\ \hline
Suma     & -7 & -14 & -6 & -14 & -4 & 0 & -6 & -51 \\ \hline
\end{tabular}
\caption{Porównanie DE/rand/1 do 14 algorytmów z BBOB 2013 w 40 wymiarach}
\end{table}

\chapter{Podsumowanie}

Cel pracy został spełniony -- poprawiono efektywność algorytmu ewolucji różnicowej dla przyjętych funkcji
proponując nowy wariant algorytmu nazwany DE/mid. Eksperymenty oraz przeprowadzone testy istotności 
statystycznej wykazały znaczną przewagę DE/mid/1 na 6 z 7 
wielowymiarowych funkcji testowych, opisanych w rozdziale \ref{sec:zestaw}. 
Ponadto, szczegółowo przeanalizowano, zaimplementowano 
i przetestowano również dwa inne, popularne warianty -- DE/rand oraz DE/best, co pozwoliło wyciągnąć 
praktyczne wnioski, np. na temat doboru liczby wektorów różnicowych $k$.
Okazało się, że dla każdego z trzech testowanych wariantów ewolucji różnicowej, zawsze
najlepiej sprawdzało się ustawienie $k = 1$.

DE/mid sprawdzał się tym lepiej w porównaniu do reszty algorytmów, im bardziej
złożony problem, tzn. im wyższy wymiar optymalizowanej funkcji. Dla małej liczby wymiarów
różnice jakości osiąganych rozwiązań zacierają się. Jednak istnieją funkcje, dla których DE/mid nie jest najlepszym wyborem. Taką funkcją okazała się np.
funkcja numer 16 z zestawu testowego -- funkcja Weierstrassa. Zatem jeśli optymalizowana funkcja celu
przypomina funkcję Weierstrassa warto użyć algorytmu DE/best/1.

Zauważono również silną korelację pomiędzy liczbą osobników poza obszarem przeszukiwań,
a jakością zwracanych rozwiązań. Algorytmy, które najlepiej umiały utrzymać się w~zbiorze 
dopuszczalnym zwracały przeważnie najlepsze rozwiązania. 

% Kierunki dalszego rozwoju

W tej pracy skoncentrowano się na modyfikacji operatora mutacji, natomiast operator krzyżowania
dla każdego algorytmu był taki sam, używano krzyżowania wymieniającego. Prawdopodobnie dokładne
przeanalizowanie wpływu krzyżowania i zaproponowanie własnych modyfikacji 
pozwoliłoby na dalsze ulepszenia algorytmów ewolucji różnicowej. 

Ze względu na ograniczenia czasowe eksperymenty były przeprowadzane na ograniczonym zbiorze
funkcji testowych, w ograniczonej liczbie wymiarów. Przetestowanie algorytmów ewolucji
różnicowej na pewnych rodzinach funkcji posiadających wspólne cechy, być może pozwoliłoby
zaobserwować wpływ cech optymalizowanej funkcji na zachowanie algorytmu. 
Otwartym i bardzo ważnym pytaniem również pozostaje jak w ogóle powinny wyglądać funkcje testowe, 
tak aby były jak najbardziej zbliżone do praktycznych zadań
optymalizacji globalnej.

Obiecującym kierunkiem rozwoju jest również adoptowanie parametrów algorytmu w~czasie
jego wykonywania, czyli tzw. samoadaptacja \cite{brest}. W momencie, gdy różnorodność populacji
algorytmu ewolucji różnicowej spadnie poniżej pewnego poziomu, algorytm przestaje badać nowe obszary.
Wówczas opłacalne byłoby zwiększenie różnorodności populacji, np. poprzez zwiększenie zasięgu mutacji.

\appendix

\chapter{Załączniki}

Do pracy została dołączona płyta CD. Zawiera one katalogi \texttt{kod/} oraz \texttt{praca/},
w~których znajduje się pełny kod źródłowy oraz ta praca w formie elektronicznej.

\chapter{Pełne wyniki eksperymentów}

\section{10 wymiarów}

\begin{table}[H]
\centering
\begin{tabular}{ | l | c | c | c | c | c | c | c | }
\hline		 & \multicolumn{7}{c |}{Numer funkcji testowej}  \\  \hline
Algorytm         &15& 16& 19& 20& 21& 22& 24 \\ \hline
DE/rand/2	 & + & + & $\cdot$ & + & $\cdot$ & $\cdot$ & $\cdot$ \\
DE/rand/6	 & + & + & $\cdot$ & + & $\cdot$ & $\cdot$ & $\cdot$ \\
DE/rand/$\infty$	 & + & + & $\cdot$ & + & $\cdot$ & $\cdot$ & $\cdot$ \\
DE/best/1	 & + & + & + & + & + & + & + \\
DE/best/2	 & + & + & $\cdot$ & + & $\cdot$ & $\cdot$ & $\cdot$ \\
DE/best/6	 & + & + & + & + & $\cdot$ & $\cdot$ & + \\
DE/best/$\infty$	 & + & + & + & + & + & $\cdot$ & + \\
DE/mid/1	 & -- & + & -- & + & $\cdot$ & $\cdot$ & -- \\
DE/mid/2	 & + & + & $\cdot$ & + & + & $\cdot$ & $\cdot$ \\
DE/mid/6	 & + & + & $\cdot$ & + & $\cdot$ & $\cdot$ & $\cdot$ \\
DE/mid/$\infty$	 & + & + & $\cdot$ & + & $\cdot$ & $\cdot$ & $\cdot$ \\ \hline
\end{tabular}
\caption{Porównanie DE/rand/1 do reszty algorytmów}
\end{table}

\begin{table}[H]
\centering
\begin{tabular}{ | l | c | c | c | c | c | c | c | }
\hline		 & \multicolumn{7}{c |}{Numer funkcji testowej}  \\  \hline
Algorytm         &15& 16& 19& 20& 21& 22& 24 \\ \hline
DE/rand/1	 & -- & -- & -- & -- & -- & -- & -- \\
DE/rand/2	 & -- & $\cdot$ & -- & + & -- & -- & -- \\
DE/rand/6	 & -- & $\cdot$ & -- & + & -- & -- & -- \\
DE/rand/$\infty$	 & -- & $\cdot$ & -- & + & -- & -- & -- \\
DE/best/2	 & -- & -- & -- & -- & -- & -- & -- \\
DE/best/6	 & $\cdot$ & + & -- & + & -- & -- & -- \\
DE/best/$\infty$	 & $\cdot$ & + & -- & + & $\cdot$ & -- & -- \\
DE/mid/1	 & -- & $\cdot$ & -- & + & -- & -- & -- \\
DE/mid/2	 & -- & $\cdot$ & -- & + & $\cdot$ & -- & -- \\
DE/mid/6	 & -- & $\cdot$ & -- & + & -- & -- & -- \\
DE/mid/$\infty$	 & -- & $\cdot$ & -- & + & -- & -- & -- \\ \hline
\end{tabular}
\caption{Porównanie DE/best/1 do reszty algorytmów}
\end{table}

\begin{table}[H]
\centering
\begin{tabular}{ | l | c | c | c | c | c | c | c | }
\hline		 & \multicolumn{7}{c |}{Numer funkcji testowej}  \\  \hline
Algorytm         &15& 16& 19& 20& 21& 22& 24 \\ \hline
DE/rand/1	 & + & -- & + & -- & $\cdot$ & $\cdot$ & + \\
DE/rand/2	 & + & + & + & -- & $\cdot$ & $\cdot$ & + \\
DE/rand/6	 & + & + & + & $\cdot$ & -- & $\cdot$ & + \\
DE/rand/$\infty$	 & + & $\cdot$ & + & $\cdot$ & $\cdot$ & $\cdot$ & + \\
DE/best/1	 & + & $\cdot$ & + & -- & + & + & + \\
DE/best/2	 & + & $\cdot$ & + & -- & $\cdot$ & $\cdot$ & + \\
DE/best/6	 & + & + & + & + & $\cdot$ & $\cdot$ & + \\
DE/best/$\infty$	 & + & + & + & + & + & $\cdot$ & + \\
DE/mid/2	 & + & + & + & $\cdot$ & $\cdot$ & $\cdot$ & + \\
DE/mid/6	 & + & + & + & $\cdot$ & $\cdot$ & $\cdot$ & + \\
DE/mid/$\infty$	 & + & + & + & $\cdot$ & $\cdot$ & $\cdot$ & + \\ \hline
\end{tabular}
\caption{Porównanie DE/mid/1 do reszty algorytmów}
\end{table}

\begin{table}[H]
\centering
\begin{tabular}{ | l | c | c | c | c | c | c | c | }
\hline		 & \multicolumn{7}{c |}{Numer funkcji testowej}  \\  \hline
Algorytm         &15& 16& 19& 20& 21& 22& 24 \\ \hline
DE/rand/1	 & 1 & 30 & 7 & 0 & 1 & 1 & 0 \\
DE/rand/2	 & 8 & 43 & 9 & 0 & 5 & 3 & 0 \\
DE/rand/6	 & 9 & 43 & 8 & 0 & 9 & 5 & 0 \\
DE/rand/$\infty$ & 9 & 43 & 10 & 0 & 10 & 7 & 0 \\
DE/best/1	 & 8 & 18 & 5 & 0 & 0 & 3 & 0 \\
DE/best/2	 & 6 & 19 & 6 & 0 & 2 & 2 & 0 \\
DE/best/6	 & 7 & 22 & 7 & 0 & 7 & 7 & 0 \\
DE/best/$\infty$ & 7 & 22 & 7 & 0 & 10 & 7 & 0 \\
DE/mid/1         & 0 & 27 & 0 & 0 & 1 & 1 & 0 \\
DE/mid/2	 & 5 & 42 & 10 & 0 & 12 & 16 & 0 \\
DE/mid/6	 & 8 & 43 & 11 & 0 & 9 & 6 & 0 \\
DE/mid/$\infty$	 & 9 & 43 & 11 & 0 & 11 & 6 & 0 \\ \hline
średnia          & 6 & 33 & 8 & 0 & 6 & 5 & 0 \\        \hline      
\end{tabular}
\caption{Średni procent osobników poza obszarem przeszukiwań}
\end{table}

\begin{figure}[H]
\centering
\includegraphics[width=.65\textwidth]{../pngs/10/15.png}
\caption{Dystrybuanty empiryczne dla 10-wymiarowej funkcji numer 15}
\end{figure}

\begin{figure}[H]
\centering
\includegraphics[width=.65\textwidth]{../pngs/10/16.png} 
\caption{Dystrybuanty empiryczne dla 10-wymiarowej funkcji numer 16}
\end{figure}

\begin{figure}[H]
\centering
\includegraphics[width=.65\textwidth]{../pngs/10/19.png}
\caption{Dystrybuanty empiryczne dla 10-wymiarowej funkcji numer 19}
\end{figure}

\begin{figure}[H]
\centering
\includegraphics[width=.65\textwidth]{../pngs/10/20.png}
\caption{Dystrybuanty empiryczne dla 10-wymiarowej funkcji numer 20}
\end{figure}

\begin{figure}[H]
\centering
\includegraphics[width=.65\textwidth]{../pngs/10/21.png}
\caption{Dystrybuanty empiryczne dla 10-wymiarowej funkcji numer 21}
\end{figure}

\begin{figure}[H]
\centering
\includegraphics[width=.65\textwidth]{../pngs/10/22.png}
\caption{Dystrybuanty empiryczne dla 10-wymiarowej funkcji numer 22}
\end{figure}

\begin{figure}[H]
\centering
\includegraphics[width=.65\textwidth]{../pngs/10/24.png}
\caption{Dystrybuanty empiryczne dla 10-wymiarowej funkcji numer 24}
\end{figure}

\section{20 wymiarów}

\begin{table}[H]
\centering
\begin{tabular}{ | l | c | c | c | c | c | c | c | }
\hline		 & \multicolumn{7}{c |}{Numer funkcji testowej}  \\  \hline
Algorytm         &15& 16& 19& 20& 21& 22& 24 \\ \hline
DE/rand/2	 & + & + & + & + & $\cdot$ & $\cdot$ & + \\
DE/rand/6	 & + & + & + & + & + & $\cdot$ & + \\
DE/rand/$\infty$	 & + & + & + & + & + & $\cdot$ & + \\
DE/best/1	 & + & + & + & + & + & + & + \\
DE/best/2	 & + & + & + & + & + & + & + \\
DE/best/6	 & + & + & + & + & + & + & + \\
DE/best/$\infty$	 & + & + & + & + & + & + & + \\
DE/mid/1	 & $\cdot$ & + & $\cdot$ & + & $\cdot$ & $\cdot$ & + \\
DE/mid/2	 & + & + & + & + & + & $\cdot$ & + \\
DE/mid/6	 & + & + & + & + & + & $\cdot$ & + \\
DE/mid/$\infty$	 & + & + & + & + & + & $\cdot$ & + \\ \hline
\end{tabular}
\caption{Porównanie DE/rand/1 do reszty algorytmów}
\end{table}

\begin{table}[H]
\centering
\begin{tabular}{ | l | c | c | c | c | c | c | c | }
\hline		 & \multicolumn{7}{c |}{Numer funkcji testowej}  \\  \hline
Algorytm         &15& 16& 19& 20& 21& 22& 24 \\ \hline
DE/rand/1	 & -- & -- & -- & -- & -- & -- & -- \\
DE/rand/2	 & $\cdot$ & + & $\cdot$ & + & -- & -- & -- \\
DE/rand/6	 & + & + & $\cdot$ & + & $\cdot$ & -- & -- \\
DE/rand/$\infty$	 & + & + & $\cdot$ & + & $\cdot$ & -- & -- \\
DE/best/2	 & + & + & + & + & $\cdot$ & $\cdot$ & + \\
DE/best/6	 & + & + & + & + & + & + & + \\
DE/best/$\infty$	 & + & + & + & + & + & + & + \\
DE/mid/1	 & -- & + & -- & + & -- & -- & -- \\
DE/mid/2	 & -- & + & -- & + & $\cdot$ & -- & -- \\
DE/mid/6	 & $\cdot$ & + & $\cdot$ & + & $\cdot$ & -- & -- \\
DE/mid/$\infty$	 & $\cdot$ & + & $\cdot$ & + & $\cdot$ & -- & -- \\ \hline
\end{tabular}
\caption{Porównanie DE/best/1 do reszty algorytmów}
\end{table}

\begin{table}[H]
\centering
\begin{tabular}{ | l | c | c | c | c | c | c | c | }
\hline		 & \multicolumn{7}{c |}{Numer funkcji testowej}  \\  \hline
Algorytm         &15& 16& 19& 20& 21& 22& 24 \\ \hline
DE/best/1	 & + & -- & + & -- & + & + & + \\
DE/best/2	 & + & + & + & + & + & + & + \\
DE/best/6	 & + & + & + & + & + & + & + \\
DE/best/$\infty$	 & + & + & + & + & + & + & + \\
DE/mid/2	 & + & + & + & $\cdot$ & + & $\cdot$ & + \\
DE/mid/6	 & + & + & + & $\cdot$ & + & $\cdot$ & + \\
DE/mid/$\infty$	 & + & + & + & $\cdot$ & + & $\cdot$ & + \\
DE/rand/1	 & $\cdot$ & -- & $\cdot$ & -- & $\cdot$ & $\cdot$ & -- \\
DE/rand/2	 & + & + & + & $\cdot$ & $\cdot$ & $\cdot$ & + \\
DE/rand/6	 & + & + & + & + & + & $\cdot$ & + \\
DE/rand/$\infty$	 & + & + & + & $\cdot$ & + & $\cdot$ & + \\ \hline
\end{tabular}
\caption{Porównanie DE/mid/1 do reszty algorytmów}
\end{table}

\begin{table}[H]
\centering
\begin{tabular}{ | l | c | c | c | c | c | c | c | }
\hline		 & \multicolumn{7}{c |}{Numer funkcji testowej}  \\  \hline
Algorytm         &15& 16& 19& 20& 21& 22& 24 \\ \hline
DE/rand/1	 & 12 & 38 & 2 & 2 & 4 & 5 & 0   \\
DE/rand/2	 & 30 & 47 & 13 & 5 & 23 & 20 & 0  \\
DE/rand/6	 & 33 & 47 & 16 & 5 & 31 & 27 & 0     \\
DE/rand/$\infty$ & 34 & 47 & 18 & 5 & 33 & 28 & 0  \\
DE/best/1	 & 8 & 21 & 1 & 0 & 1 & 2 & 0      \\
DE/best/2	 & 21 & 22 & 11 & 8 & 15 & 16 & 0   \\
DE/best/6	 & 23 & 21 & 15 & 13 & 15 & 15 & 1      \\
DE/best/$\infty$ & 23 & 21 & 14 & 12 & 15 & 16 & 1 \\
DE/mid/1         & 3 & 42 & 0 & 0 & 0 & 0 & 0  \\
DE/mid/2	 & 23 & 47 & 9 & 3 & 25 & 18 & 0  \\
DE/mid/6	 & 32 & 47 & 18 & 5 & 31 & 26 & 0  \\
DE/mid/$\infty$	 & 32 & 47 & 17 & 4 & 32 & 25 & 0 \\ \hline
średnia          & 23 & 37 & 11 & 5 & 19 & 17 & 0 \\  \hline
\end{tabular}
\caption{Średni \% osobników poza obszarem przeszukiwań}
\end{table}

\begin{figure}[H]
\centering
\includegraphics[width=.65\textwidth]{../pngs/20/15.png}
\caption{Dystrybuanty empiryczne dla 20-wymiarowej funkcji numer 15}
\end{figure}

\begin{figure}[H]
\centering
\includegraphics[width=.65\textwidth]{../pngs/20/16.png} 
\caption{Dystrybuanty empiryczne dla 20-wymiarowej funkcji numer 16}
\end{figure}

\begin{figure}[H]
\centering
\includegraphics[width=.65\textwidth]{../pngs/20/19.png}
\caption{Dystrybuanty empiryczne dla 20-wymiarowej funkcji numer 19}
\end{figure}

\begin{figure}[H]
\centering
\includegraphics[width=.65\textwidth]{../pngs/20/20.png}
\caption{Dystrybuanty empiryczne dla 20-wymiarowej funkcji numer 20}
\end{figure}

\begin{figure}[H]
\centering
\includegraphics[width=.65\textwidth]{../pngs/20/21.png}
\caption{Dystrybuanty empiryczne dla 20-wymiarowej funkcji numer 21}
\end{figure}

\begin{figure}[H]
\centering
\includegraphics[width=.65\textwidth]{../pngs/20/22.png}
\caption{Dystrybuanty empiryczne dla 20-wymiarowej funkcji numer 22}
\end{figure}

\begin{figure}[H]
\centering
\includegraphics[width=.65\textwidth]{../pngs/20/24.png}
\caption{Dystrybuanty empiryczne dla 20-wymiarowej funkcji numer 24}
\end{figure}

\section{40 wymiarów}

\begin{table}[H]
\centering
\begin{tabular}{ | l | c | c | c | c | c | c | c | }
\hline		 & \multicolumn{7}{c |}{Numer funkcji testowej}  \\  \hline
Algorytm         &15& 16& 19& 20& 21& 22& 24 \\ \hline
DE/rand/2	 & + & + & + & + & + & + & + \\
DE/rand/6	 & + & + & + & + & + & + & + \\
DE/rand/$\infty$	 & + & + & + & + & + & + & + \\
DE/best/1	 & + & -- & + & + & + & + & + \\
DE/best/2	 & + & + & + & + & + & + & + \\
DE/best/6	 & + & + & + & + & + & + & + \\
DE/best/$\infty$	 & + & + & + & + & + & + & + \\
DE/mid/1	 & -- & $\cdot$ & -- & + & $\cdot$ & $\cdot$ & -- \\
DE/mid/2	 & + & + & + & + & + & + & + \\
DE/mid/6	 & + & + & + & + & + & + & + \\
DE/mid/$\infty$	 & + & + & + & + & + & + & + \\ \hline
\end{tabular}
\caption{Porównanie DE/rand/1 do reszty algorytmów}
\end{table}

\begin{table}[H]
\centering
\begin{tabular}{ | l | c | c | c | c | c | c | c | }
\hline		 & \multicolumn{7}{c |}{Numer funkcji testowej}  \\  \hline
Algorytm         &15& 16& 19& 20& 21& 22& 24 \\ \hline
DE/rand/1	 & -- & + & -- & -- & -- & -- & -- \\
DE/rand/2	 & + & + & + & + & + & + & + \\
DE/rand/6	 & + & + & + & + & + & + & + \\
DE/rand/$\infty$	 & + & + & + & + & + & + & + \\
DE/best/2	 & + & + & + & + & + & + & + \\
DE/best/6	 & + & + & + & + & + & + & + \\
DE/best/$\infty$	 & + & + & + & + & + & + & + \\
DE/mid/1	 & -- & + & -- & -- & -- & -- & -- \\
DE/mid/2	 & -- & + & $\cdot$ & -- & $\cdot$ & $\cdot$ & -- \\
DE/mid/6	 & + & + & + & + & + & + & + \\
DE/mid/$\infty$	 & + & + & + & + & + & + & + \\ \hline
\end{tabular}
\caption{Porównanie DE/best/1 do reszty algorytmów}
\end{table}

\begin{table}[H]
\centering
\begin{tabular}{ | l | c | c | c | c | c | c | c | }
\hline		 & \multicolumn{7}{c |}{Numer funkcji testowej}  \\  \hline
Algorytm         &15& 16& 19& 20& 21& 22& 24 \\ \hline
DE/rand/1	 & + & $\cdot$ & + & -- & $\cdot$ & $\cdot$ & + \\
DE/rand/2	 & + & + & + & + & + & + & + \\
DE/rand/6	 & + & + & + & + & + & + & + \\
DE/rand/$\infty$	 & + & + & + & + & + & + & + \\
DE/best/1	 & + & -- & + & + & + & + & + \\
DE/best/2	 & + & + & + & + & + & + & + \\
DE/best/6	 & + & + & + & + & + & + & + \\
DE/best/$\infty$	 & + & + & + & + & + & + & + \\
DE/mid/2	 & + & + & + & + & + & + & + \\
DE/mid/6	 & + & + & + & + & + & + & + \\
DE/mid/$\infty$	 & + & + & + & + & + & + & + \\ \hline
\end{tabular}
\caption{Porównanie DE/mid/1 do reszty algorytmów}
\end{table}

\begin{table}[H]
\centering
\begin{tabular}{ | l | c | c | c | c | c | c | c | }
\hline		 & \multicolumn{7}{c |}{Numer funkcji testowej}  \\  \hline
Algorytm         &15& 16& 19& 20& 21& 22& 24 \\ \hline
DE/rand/1	 & 31 & 42 & 3 & 6 & 17 & 16 & 0   \\
DE/rand/2	 & 47 & 42 & 35 & 45 & 14 & 12 & 2   \\
DE/rand/6	 & 44 & 41 & 35 & 43 & 4 & 3 & 0      \\
DE/rand/$\infty$ & 43 & 40 & 33 & 42 & 3 & 2 & 0   \\
DE/best/1	 & 24 & 19 & 0 & 1 & 4 & 11 & 0    \\
DE/best/2	 & 20 & 18 & 10 & 19 & 0 & 0 & 0    \\
DE/best/6	 & 17 & 16 & 6 & 15 & 0 & 0 & 0      \\
DE/best/$\infty$ & 16 & 16 & 6 & 15 & 0 & 0 & 0  \\
DE/mid/1         & 7 & 45 & 0 & 0 & 1 & 1 & 0  \\
DE/mid/2	 & 46 & 43 & 17 & 26 & 26 & 25 & 2   \\
DE/mid/6	 & 44 & 40 & 35 & 43 & 4 & 3 & 0     \\
DE/mid/$\infty$	 & 43 & 38 & 35 & 44 & 5 & 3 & 0    \\ \hline
średnia          & 32 & 33 & 18 & 25 & 7 & 6 & 0 \\  \hline
\end{tabular}
\caption{Średni \% osobników poza obszarem przeszukiwań}
\end{table}

\begin{figure}[H]
\centering
\includegraphics[width=.65\textwidth]{../pngs/40/15.png}
\caption{Dystrybuanty empiryczne dla 40-wymiarowej funkcji numer 15}
\end{figure}

\begin{figure}[H]
\centering
\includegraphics[width=.65\textwidth]{../pngs/40/16.png} 
\caption{Dystrybuanty empiryczne dla 40-wymiarowej funkcji numer 16}
\end{figure}

\begin{figure}[H]
\centering
\includegraphics[width=.65\textwidth]{../pngs/40/19.png}
\caption{Dystrybuanty empiryczne dla 40-wymiarowej funkcji numer 19}
\end{figure}

\begin{figure}[H]
\centering
\includegraphics[width=.65\textwidth]{../pngs/40/20.png}
\caption{Dystrybuanty empiryczne dla 40-wymiarowej funkcji numer 20}
\end{figure}

\begin{figure}[H]
\centering
\includegraphics[width=.65\textwidth]{../pngs/40/21.png}
\caption{Dystrybuanty empiryczne dla 40-wymiarowej funkcji numer 21}
\end{figure}

\begin{figure}[H]
\centering
\includegraphics[width=.65\textwidth]{../pngs/40/22.png}
\caption{Dystrybuanty empiryczne dla 40-wymiarowej funkcji numer 22}
\end{figure}

\begin{figure}[H]
\centering
\includegraphics[width=.65\textwidth]{../pngs/40/24.png}
\caption{Dystrybuanty empiryczne dla 40-wymiarowej funkcji numer 24}
\end{figure}

\section{80 wymiarów}

\begin{table}[H]
\centering
\begin{tabular}{ | l | c | c | c | c | c | c | c | }
\hline		 & \multicolumn{7}{c |}{Numer funkcji testowej}  \\  \hline
Algorytm         &15& 16& 19& 20& 21& 22& 24 \\ \hline
DE/rand/2	 & + & + & + & + & + & + & + \\
DE/rand/6	 & + & + & + & + & + & + & + \\
DE/rand/$\infty$	 & + & + & + & + & + & + & + \\
DE/best/1	 & + & -- & + & + & + & $\cdot$ & + \\
DE/best/2	 & + & + & + & + & + & + & + \\
DE/best/6	 & + & + & + & + & + & + & + \\
DE/best/$\infty$	 & + & + & + & + & + & + & + \\
DE/mid/1	 & -- & $\cdot$ & -- & -- & -- & -- & -- \\
DE/mid/2	 & + & + & + & + & + & + & + \\
DE/mid/6	 & + & + & + & + & + & + & + \\
DE/mid/$\infty$	 & + & + & + & + & + & + & + \\ \hline
\end{tabular}
\caption{Porównanie DE/rand/1 do reszty algorytmów}
\end{table}

\begin{table}[H]
\centering
\begin{tabular}{ | l | c | c | c | c | c | c | c | }
\hline		 & \multicolumn{7}{c |}{Numer funkcji testowej}  \\  \hline
Algorytm         &15& 16& 19& 20& 21& 22& 24 \\ \hline
DE/rand/1	 & -- & + & -- & -- & -- & $\cdot$ & -- \\
DE/rand/2	 & + & + & + & + & + & + & + \\
DE/rand/6	 & + & + & + & + & + & + & + \\
DE/rand/$\infty$	 & + & + & + & + & + & + & + \\
DE/best/2	 & + & + & + & + & + & + & + \\
DE/best/6	 & + & + & + & + & + & + & + \\
DE/best/$\infty$	 & + & + & + & + & + & + & + \\
DE/mid/1	 & -- & + & -- & -- & -- & -- & -- \\
DE/mid/2	 & + & + & -- & -- & + & + & + \\
DE/mid/6	 & + & + & + & + & + & + & + \\
DE/mid/$\infty$	 & + & + & + & + & + & + & + \\ \hline

\end{tabular}
\caption{Porównanie DE/best/1 do reszty algorytmów}
\end{table}

\begin{table}[H]
\centering
\begin{tabular}{ | l | c | c | c | c | c | c | c | }
\hline		 & \multicolumn{7}{c |}{Numer funkcji testowej}  \\  \hline
Algorytm         &15& 16& 19& 20& 21& 22& 24 \\ \hline
DE/rand/1	 & + & $\cdot$ & + & + & + & + & + \\
DE/rand/2	 & + & + & + & + & + & + & + \\
DE/rand/6	 & + & + & + & + & + & + & + \\
DE/rand/$\infty$	 & + & + & + & + & + & + & + \\
DE/best/1	 & + & -- & + & + & + & + & + \\
DE/best/2	 & + & + & + & + & + & + & + \\
DE/best/6	 & + & + & + & + & + & + & + \\
DE/best/$\infty$	 & + & + & + & + & + & + & + \\
DE/mid/2	 & + & + & + & + & + & + & + \\
DE/mid/6	 & + & + & + & + & + & + & + \\
DE/mid/$\infty$	 & + & + & + & + & + & + & + \\ \hline
\end{tabular}
\caption{Porównanie DE/mid/1 do reszty algorytmów}
\end{table}

\begin{table}[H]
\centering
\begin{tabular}{ | l | c | c | c | c | c | c | c | }
\hline		 & \multicolumn{7}{c |}{Numer funkcji testowej}  \\  \hline
Algorytm         &15& 16& 19& 20& 21& 22& 24 \\ \hline
DE/rand/1	 & 40 & 39 & 10 & 20 & 16 & 17 & 1    \\
DE/rand/2	 & 39 & 36 & 18 & 32 & 0 & 0 & 0  \\
DE/rand/6	 & 31 & 32 & 4 & 21 & 0 & 0 & 0        \\
DE/rand/$\infty$ & 30 & 30 & 3 & 19 & 0 & 0 & 0     \\
DE/best/1	 & 22 & 20 & 1 & 2 & 11 & 10 & 0   \\
DE/best/2	 & 12 & 14 & 1 & 8 & 0 & 0 & 0     \\
DE/best/6	 & 8 & 11 & 0 & 4 & 0 & 0 & 0        \\
DE/best/$\infty$ & 8 & 9 & 0 & 4 & 0 & 0 & 0      \\
DE/mid/1         & 14 & 46 & 1 & 1 & 2 & 3 & 0   \\
DE/mid/2	 & 45 & 39 & 30 & 40 & 1 & 0 & 0      \\
DE/mid/6	 & 32 & 30 & 4 & 22 & 0 & 0 & 0     \\
DE/mid/$\infty$	 & 28 & 26 & 3 & 19 & 0 & 0 & 0     \\ \hline
średnia          & 26 & 28 & 6 & 16 & 3 & 3 & 0 \\    \hline
\end{tabular}
\caption{Średni \% osobników poza obszarem przeszukiwań}
\end{table}

\begin{figure}[H]
\centering
\includegraphics[width=.65\textwidth]{../pngs/80/15.png}
\caption{Dystrybuanty empiryczne dla 80-wymiarowej funkcji numer 15}
\end{figure}

\begin{figure}[H]
\centering
\includegraphics[width=.65\textwidth]{../pngs/80/16.png} 
\caption{Dystrybuanty empiryczne dla 80-wymiarowej funkcji numer 16}
\end{figure}

\begin{figure}[H]
\centering
\includegraphics[width=.65\textwidth]{../pngs/80/19.png}
\caption{Dystrybuanty empiryczne dla 80-wymiarowej funkcji numer 19}
\end{figure}

\begin{figure}[H]
\centering
\includegraphics[width=.65\textwidth]{../pngs/80/20.png}
\caption{Dystrybuanty empiryczne dla 80-wymiarowej funkcji numer 20}
\end{figure}

\begin{figure}[H]
\centering
\includegraphics[width=.65\textwidth]{../pngs/80/21.png}
\caption{Dystrybuanty empiryczne dla 80-wymiarowej funkcji numer 21}
\end{figure}

\begin{figure}[H]
\centering
\includegraphics[width=.65\textwidth]{../pngs/80/22.png}
\caption{Dystrybuanty empiryczne dla 80-wymiarowej funkcji numer 22}
\end{figure}

\begin{figure}[H]
\centering
\includegraphics[width=.65\textwidth]{../pngs/80/24.png}
\caption{Dystrybuanty empiryczne dla 80-wymiarowej funkcji numer 24}
\end{figure}

\nocite{*}
\bibliographystyle{plplain}
\bibliography{references}

\end{document}
